{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974bc7b5",
   "metadata": {},
   "source": [
    "# Organize with EDA\n",
    "I'm taking a look only for title, but eventually, both title and text will be used.\n",
    "- For future: mind dots (handle dots without hurting sentence tokenization) in text\n",
    "- Ignore comment about text for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fea3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freq_utils import *\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.help import upenn_tagset\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b49ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_org = pd.read_csv('data/True.csv')\n",
    "df1_org = pd.read_csv('data/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260fa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data we don't use (from eda_raw.ipynb)\n",
    "df0_org.drop(['text','subject','date'], axis=1, inplace=True)\n",
    "df1_org.drop(['text','subject','date'], axis=1, inplace=True)\n",
    "\n",
    "df0_org.drop_duplicates()\n",
    "df1_org.drop_duplicates()\n",
    "\n",
    "df1_org = df1_org[df1_org.title.str.split().str.len()>2]\n",
    "#df0 = df0[df0.text.str.split().str.len()>19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c7560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare modification result\n",
    "df0 = df0_org\n",
    "df1 = df1_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef42714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences_with_this_string(this_string, column_to_look, df_list, df_names, \n",
    "                                     print_words=False, print_set=False, sent_token=False):\n",
    "    \n",
    "    n_dataFrame = len(df_list)\n",
    "    \n",
    "    pat = re.compile(this_string)\n",
    "    \n",
    "    set_list = []\n",
    "    \n",
    "    for i in range(n_dataFrame):\n",
    "        df = df_list[i]\n",
    "        df = df[df[column_to_look].str.contains(this_string, regex= True, na=False)]\n",
    "        \n",
    "        count = df[column_to_look].count()\n",
    "        \n",
    "        \n",
    "        print(this_string,'in',column_to_look,'\\n',df_names[i],':',count)\n",
    "        \n",
    "        if count==0:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if print_set:\n",
    "            df = df.sample(min(len(df),1000), random_state=20)\n",
    "        else:\n",
    "            df = df.sample(min(len(df),20), random_state=20)\n",
    "        \n",
    "        corpus_list = df[column_to_look].to_numpy()\n",
    "        index_list  = df.index.to_numpy()\n",
    "                \n",
    "        example_df = pd.DataFrame(columns=['index','selected_text','selected_words'])\n",
    "        \n",
    "      \n",
    "        for row in range(len(index_list)):\n",
    "            \n",
    "            if sent_token:\n",
    "                sentences = sent_tokenize(corpus_list[row])\n",
    "            \n",
    "                display_text = ''\n",
    "                display_word = []\n",
    "            \n",
    "                for sentence in sentences:\n",
    "                \n",
    "                    if pat.search(sentence):\n",
    "                        display_text += sentence+' '\n",
    "                        display_word += pat.findall(sentence)\n",
    "                                        \n",
    "                example_df.loc[row] = [index_list[row],display_text,display_word]\n",
    "            else:\n",
    "                if pat.search(corpus_list[row]):\n",
    "               \n",
    "                    display_text = corpus_list[row]\n",
    "                    display_word = pat.findall(display_text)\n",
    "                    example_df.loc[row] = [index_list[row],display_text,display_word]\n",
    "                \n",
    "            \n",
    "            \n",
    "        example_df.set_index('index')\n",
    "        \n",
    "        if print_set:    \n",
    "            #word_set = set()\n",
    "            word_set = list()\n",
    "            \n",
    "            lst_list = list(example_df.selected_words)\n",
    "            \n",
    "            for lst in lst_list:\n",
    "                word_set += lst\n",
    "                \n",
    "            #print(word_set)\n",
    "            \n",
    "            word_counter = Counter(word_set)\n",
    "            print(word_counter.most_common(200))\n",
    "        \n",
    "\n",
    "        if not print_words:\n",
    "            example_df.drop(['selected_words'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        display(example_df.sample(min(len(df),20), random_state=20))\n",
    "        \n",
    "        if print_set:\n",
    "            set_list.append(word_set)\n",
    "            \n",
    "    if print_set:\n",
    "        return set_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb7c7",
   "metadata": {},
   "source": [
    "# Digital source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db81896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*[@]+[^\\s]*',        \n",
    "                                 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*//[^\\s]+[.][^\\s]+', \n",
    "                                 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678e876",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- Overall, **fake news quote digital sources much more frequently** than real news. \n",
    "\n",
    "#### @ in title\n",
    "- Real news: only one news has it, when its **topic is about social media account**\n",
    "- Fake news: some are about **social media account**, but some are **slangs** (used like \"*\")\n",
    "\n",
    "#### @ in text\n",
    "- Both real and fake news have @ to **refer social media accounts**.\n",
    "- 20 times **more frequently** used in fake news\n",
    "\n",
    "#### website in text\n",
    "- **Few real news** contains the website address in this dataset.\n",
    "- **A lot of fake news** contains website address. Examples of them were CNN news, Facebook, and YouTube address.\n",
    "\n",
    "### Processing\n",
    "- There are only a few rows, so let's simply change **@ to _**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65840a44",
   "metadata": {},
   "source": [
    "### Replace them to tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c767ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.replace(to_replace='@', value='_mytag_at_', regex=True, inplace=False)\n",
    "df1 = df1.replace(to_replace='@', value='_mytag_at_', regex=True, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a3ed1",
   "metadata": {},
   "source": [
    "# Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37712e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*[\\*]+[^\\s]*', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10355b2",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- No real news has * in titles.\n",
    "- Some **fake news** have * in **title** to display **slangs**.\n",
    "- Both real and fake news have **\\* in texts**, 14 times **frequently occur in fake news**.\n",
    "- When * is used **in the text**, it is **not always for slangs** (e.g. to emphasize). \n",
    "- It is not **hard to separate** usage of star between **slang and highlighting** based on text pattern.\n",
    "- I'll mark both of those works as `_mytag_slang_` since they have a common meaning and function, **highlighting, anyway.\n",
    "\n",
    "### Processing\n",
    "- Tag words contain * as **slang** (only for title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6631cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace works with *\n",
    "df0.replace(to_replace='[^\\s]*[\\*]+[^\\s]*', value='_mytag_slang_', regex=True, inplace=True)\n",
    "df1.replace(to_replace='[^\\s]*[\\*]+[^\\s]*', value='_mytag_slans_', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc4d98",
   "metadata": {},
   "source": [
    "# Other special characters\n",
    "As seen from the slang character tagging, some special character replaces an alphabet character, therefore, blindly removing all special characters might leave some words meaningless.\n",
    "Let's check how the other special characters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd947dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "sc_title = print_sentences_with_this_string('[^\\s\\w]', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a27dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "c0 = Counter(sc_title[0])\n",
    "c1 = Counter(sc_title[1])\n",
    "\n",
    "print(c0)\n",
    "print(c1)\n",
    "\n",
    "print(set([x[0] for x in c0.most_common(5)]))\n",
    "\n",
    "\n",
    "sc_fake_only = set([x[0] for x in c1]) - set([x[0] for x in c0.most_common(5)])\n",
    "\n",
    "\n",
    "print(sc_fake_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c6c4",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Fake news are more noisy having more kind of special characters. Special characters not used in real news might have a special function. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef95291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for x in c0:\n",
    "    \n",
    "    #sc_regex = '[\\\\' + x + ']'\n",
    "    sc_regex = '[^\\s]*[\\\\' + x + '][^\\s]*'\n",
    "\n",
    "    print(x)\n",
    "    print_sentences_with_this_string(sc_regex, 'title', [df0, df1], ['Real','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3741b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for x in sc_fake_only:\n",
    "    \n",
    "    #sc_regex = '[\\\\' + x + ']'\n",
    "    sc_regex = '[^\\s]*[\\\\' + x + '][^\\s]*'\n",
    "\n",
    "    print(x)\n",
    "    print_sentences_with_this_string(sc_regex, 'title', [df1], ['Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ce59b",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Usages\n",
    "- \\# : hashtag, tv show episode, website address \n",
    "- % : percent, not used in real news, interestingly\n",
    "- -- (longer than a hyphen) : hyphen, some slang but ignorable\n",
    "- ! : exclamation, not used in real news, but remove it in case real news happen to have an exclamation mark is classified as fake\n",
    "- [] (): clickbait, emphasis\n",
    "- } : seems a typo of ] in case of title, in text, it looks like a script. Token with } in text better be removed.\n",
    "- & : and or special words (e.g. Q&A, AT&T)\n",
    "- \\$ : dollor or slang\n",
    "- \\/ : 9/11, 24/7, or clickbait (e.g. video/image)\n",
    "\n",
    "### Note for processings\n",
    "\n",
    "- \\/ : **replace to a space*\n",
    "- [] () {} : **remove with enclosed text** to avoid a strong bias of this dataset\n",
    "- : : **replace to a space** if it is between two numbers (time), **replace to ;** otherwise \n",
    "- ;, ... : **replace to \\.**\n",
    "- Abc. (abbreviation has one dot at the end): **remove dot**\n",
    "\n",
    "Tokenize sentence. Then\n",
    "\n",
    "- — : **replace to -** then do the same as below\n",
    "- \\- : **leave it** if it is hyphen (between two words without space), **remove** otherwise\n",
    "- \\$ : **remove** if followed by a number, **replace to \\_** otherwise (slangs)\n",
    "- \\& : **replace to \"and\"** if spaced, **replace to \\_ otherwise\"\n",
    "- \\% : **replace to \" percent \"**\n",
    "\n",
    "- \\# : remove (words are either special noun or number)\n",
    "- !, ?, , : remove\n",
    "- \\\" : remove\n",
    "\n",
    "Tokenize word. Then\n",
    "- \\' : remove\n",
    "- handle abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5362de5",
   "metadata": {},
   "source": [
    "# Capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8772d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print_sentences_with_this_string('[\\s^\\w][A-Z][^\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdbdb1",
   "metadata": {},
   "source": [
    "### Findings\n",
    "As from EDA, words with capicalized first characters are proper nouns. Name entity recognition would recognize some of them (e.g. \"Trump\"), but some woudn't (e.g. \"White\", \"House\"). However, bigram or trigram would catch such case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4abb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "title0 = df0.sample(1000, random_state=9).title.tolist()\n",
    "title1 = df1.sample(1000, random_state=9).title.tolist()\n",
    "\n",
    "# Make a bigram list\n",
    "# Subtract trigram\n",
    "bi_real = ngram_tokenizer(title0, n=2)\n",
    "bi_fake = ngram_tokenizer(title1, n=2)\n",
    "tr_real = ngram_tokenizer(title0, n=3)\n",
    "tr_fake = ngram_tokenizer(title1, n=3)\n",
    "qd_real = ngram_tokenizer(title0, n=4)\n",
    "qd_fake = ngram_tokenizer(title1, n=4)\n",
    "\n",
    "def combine_uppercase_words(lst):\n",
    "\n",
    "    rg = re.compile('[A-Z]')\n",
    "    \n",
    "    co = Counter(lst)  \n",
    "    ngram_lst  = [x[0] for x in list(co.most_common(100))]\n",
    "    \n",
    "    lst_combine=[]\n",
    "\n",
    "    for x in ngram_lst:\n",
    "        # x: bigram or trigram tuple\n",
    "        ngram = len(x)\n",
    "        \n",
    "        if set([bool(rg.search(x[i][0])) for i in range(ngram)])=={True}:\n",
    "            lst_combine.append(x)\n",
    "    \n",
    "    return ngram_lst, lst_combine\n",
    "            \n",
    "print(combine_uppercase_words(bi_real),'\\n')            \n",
    "print(combine_uppercase_words(tr_real),'\\n')            \n",
    "print(combine_uppercase_words(qd_real),'\\n')            \n",
    "print(combine_uppercase_words(bi_fake),'\\n')            \n",
    "print(combine_uppercase_words(tr_fake),'\\n')            \n",
    "print(combine_uppercase_words(qd_fake),'\\n') \n",
    "\n",
    "bi = combine_uppercase_words(bi_real)[1]\n",
    "tr = combine_uppercase_words(tr_real)[1]\n",
    "\n",
    "for t in tr:\n",
    "    for b in bi:\n",
    "        if set(b).issubset(set(t)):\n",
    "            print(b, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc21f91",
   "metadata": {},
   "source": [
    "### Note for preprocessings\n",
    "- Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c775aa",
   "metadata": {},
   "source": [
    "# Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23a53d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# US, USA, UN, UK... -> Add two \"_\" at the end\n",
    "print_sentences_with_this_string('[A-Z][A-Z]+[\\.]?', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd7b6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Abbreviation with a space between, like U. S.?\n",
    "print_sentences_with_this_string('[A-Z][\\w]*[.][\\s][A-Z][\\w]*[.]', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4336b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Abbreviation with lower cases?\n",
    "print_sentences_with_this_string('[a-z][\\w]*[.][a-z][\\w]*[.][^\\s]*', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d16bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# U.S., Dr.\n",
    "print_sentences_with_this_string('[A-Z][\\w]*[\\.][\\w\\.]*', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2671f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# single character words\n",
    "print_sentences_with_this_string('[\\s]+[A-Z][\\.][\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a83509",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# two character words\n",
    "print_sentences_with_this_string('[\\s]+[A-Z][\\.]?[A-Z][\\.]?[\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16828b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Words have only one dot at the end\n",
    "print_sentences_with_this_string('[A-Z][\\w]+[\\.][\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa8048e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Check other dot examples\n",
    "print_sentences_with_this_string('[^\\s]*[\\.][^\\s]*', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2266261",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- The dot in any abbreviation will interrupt sentence tokenization.\n",
    "    - Abbreviation at the end of a sentence\n",
    "        - 'I'm in the **U.S.** That is a news!' tokenized into **one** sentence.\n",
    "        - 'I'm in the **Dept.** That is a news!' tokenized into **two** sentences.\n",
    "    - Abbreviation in the middle of a sentence\n",
    "        - 'I'm in the **U.S.** now!' tokenized into **one** sentence.\n",
    "        - 'I'm in the **Dept.** now!' tokenized into **two** sentences.\n",
    "\n",
    "- Due to **irregular capitalization/formatting rules in fake news**, distinguish these words relying on text format **without context** seems **impossible**.\n",
    "- We can ignore possible spaces between abbreviation because they didn't happen (e.g. U. S.).\n",
    "- May (month) vs may (modal verb) vs May (name) is hard to distinguish. I'll leave it up to the learning a context.\n",
    "\n",
    "Here is a note about most frequent words.\n",
    "\n",
    "\n",
    "#### Bigram/Trigram\n",
    "- Some words start with upper case should be combined to have meaning (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)\n",
    "- No point to use fake news bi/trigram to replace Capital+Capical words because it is full of noise.\n",
    "- For 'House', 'Speaker' and 'Ryan', 'Ryan' should be separated.\n",
    "- For now, let's leave it up to learning\n",
    "\n",
    "#### Examples of words that should be recognized as a same word\n",
    "- **US, U.S, U.S. or U.S.A.**: variation of the United States, comes from fake news or typo. \n",
    "\n",
    "#### Examples of words that should be recognized as distinct words\n",
    "- **PM (Prime Minister)**, **P.M. (Post Meridiem)**, and **p.m. (post meridiem)**\n",
    "- **IS (Islamic State)** and **is (be verb)**\n",
    "- **No. (number)** and **no (opposite of yes)**\n",
    "- **IT (information technology) ** and **it (pronoun)**\n",
    "\n",
    "#### Single or two characters words\n",
    "- They can be removed by **stop word** removal. \n",
    "- Most of **single letters** for middle name except **N. in North Korea**. It's ok to drop middle names.\n",
    "- For **two characters** words, add extra \\_ at the end in order not to be disappeared.\n",
    "\n",
    "#### Words have only one dot at the end\n",
    "- These words can distort sentence tokenization.\n",
    "- They are followed by a proper noun (**Dr., Sen., Jr.**), which is ok to be **removed**, or name of month (**Jul.**). Either case, **removing dot** would be enough.\n",
    "\n",
    "### Note for preprocessings\n",
    "- Change **N. Korea**, **N.Korea** to **North Korea**\n",
    "- Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)\n",
    "- Change abbreviations\n",
    "    - U.N. : \\_u_n_\n",
    "    - Rep. : Rep\n",
    "    - Sept. : Sep\n",
    "    - Sen. : Sen\n",
    "    - Gov. : Gov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c497dd",
   "metadata": {},
   "source": [
    "# Preprocessings\n",
    "\n",
    "#### 1. Least interfering preprocessings\n",
    "\n",
    "1. Replace N. Korea, N.Korea to North Korea (frequently occuring topic)\n",
    "2. Remove single letter capical word (e.g. middle name)\n",
    "3. Abbreviation\n",
    "    - Remove a dot at the end of a Month word starts with an upper case. (Jul. -> Jul)\n",
    "    - No. to number\n",
    "    - U.N. : \\_u_n_\n",
    "    - Rep. : Rep\n",
    "    - Sept. : Sep\n",
    "    - Sen. : Sen\n",
    "    - Gov. : Gov\n",
    "    - PM : \\_p_m_\n",
    "    - P.M. (Post Meridiem), and p.m. (post meridiem): \\_mytag_pm_ (same for a.m.)\n",
    "    - US, U.S, U.S. or U.S.A.: \\_u_s_\n",
    "4. Special characters \n",
    "    - / : a space\n",
    "    - [] () {} : \\_mytag_parentheses_\n",
    "    - $ : remove (not care about a few usages for slang)\n",
    "    - & : replace to \"and\" if spaced, replace to an underbar otherwise\n",
    "    - % : replace to \" percent \"\n",
    "    - \\# : replace to an underbar  (words are either special noun or number)\n",
    "5. Special characters (after abbreviation handling)\n",
    "    - ... : a space\n",
    "    - : : ~replace to a space if it is between two numbers (time), replace to . otherwise~ leave it to tokenizer\n",
    "    - ; : ~replace to \\.~ leave it to tokenizer\n",
    "\n",
    "\n",
    "#### ~2. After sentence tokenization~ No sentenct tokenization for title\n",
    "1. ~Bigram: words that have different meaning if used alone: Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong) with an underbar, keep capicalization~ leave it to learning\n",
    "        \n",
    "\n",
    "\n",
    "#### 3. After word tokenization\n",
    "1. Special characters\n",
    "    - — (en dash? em dash?): replace to - (hyphen) then do the same as below\n",
    "    - \\- : replace it to an underbar if it is hyphen (between two words without space), remove otherwise\n",
    "    - !, ?, , : remove\n",
    "    - \\\" : remove\n",
    "2. Abbreviation: AB, A.B -> \\_a_b_\n",
    "\n",
    "\n",
    "#### 4. After PoS tagging\n",
    "    - \\' : remove    \n",
    "    - Uncapitalization\n",
    "\n",
    "<Text items >\n",
    "<Remove news id (location, reuters) >\n",
    "<Replace month abbreviations>\n",
    "<july, jul., Jul, Jul., July : \\_mytag_month_july_ >\n",
    "<May: \\_mytag_month_may_ >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1d518",
   "metadata": {},
   "source": [
    "## Least interfering preprocessings\n",
    "Includes handlings for better sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b227d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_regex(old_exp, new_exp='', verbose=True):\n",
    "    \n",
    "    if verbose:\n",
    "        print_sentences_with_this_string(old_exp, 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n",
    "    \n",
    "    if not new_exp=='':\n",
    "        df0.replace(to_replace=old_exp, value=new_exp, regex=True, inplace=True)\n",
    "        df1.replace(to_replace=old_exp, value=new_exp, regex=True, inplace=True)\n",
    "        if verbose:\n",
    "            print(old_exp,'replaced to',new_exp)\n",
    "            print_sentences_with_this_string(new_exp, 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede65d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace N. Korea, N.Korea to North Korea\n",
    "old_exp = '(?:[\\s]|^)[N][\\.][\\s]?Korea'\n",
    "new_exp = ' North Korea'\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6b2db47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove single letter capical word (e.g. middle name)\n",
    "old_exp = '(?:[\\s]|^)[A-Z][\\.](?:[\\s]|$)'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0223ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. to No\n",
    "# U.N. : _u_n_\n",
    "# Rep. : Rep\n",
    "# Sept. : Sep\n",
    "# Sen. : Sen\n",
    "# Gov. : Gov\n",
    "# PM : \\_p_m_\n",
    "# **P.M. (Post Meridiem)**, and **p.m. (post meridiem)**: \\_mytag_pm_ (same for a.m.)\n",
    "# **US, U.S, U.S. or U.S.A.**: \\_u_s_\n",
    "\n",
    "      \n",
    "#old_exp = '(?:[\\s]|^)[N][o][\\.]'\n",
    "#new_exp = ' No '\n",
    "old_exp = '(?:[\\s]|^)[N][o][\\.]'\n",
    "new_exp = ' No. '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[U][\\.]?[N][\\.]?(?:[\\s]|$)'\n",
    "#new_exp = ' _u_n_ '\n",
    "new_exp = ' UN '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Rr][Ee][Pp][\\.]'\n",
    "new_exp = ' Rep '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Pp][Tt][\\.]'\n",
    "new_exp = ' Sept '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Nn][\\.]'\n",
    "new_exp = ' Sen '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Gg][Oo][Vv][\\.]'\n",
    "new_exp = ' Gov '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[P][M](?:[\\s]|$)'\n",
    "#new_exp = ' _p_m_ '\n",
    "new_exp = ' PM '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Pp][\\.][Mm][\\.](?:[\\s]|$)'\n",
    "#new_exp = ' _mytag_pm_ '\n",
    "new_exp = ' p.m. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Aa][\\.][Mm][\\.](?:[\\s]|$)'\n",
    "#new_exp = ' _mytag_am_ '\n",
    "new_exp = ' a.m. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[U][\\.]?[S][\\.]?[A]?[\\.]?(?:[\\s]|$)'\n",
    "#new_exp = ' _u_s_ '\n",
    "new_exp = ' U.S. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fffd5571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove a dot at the end of a word starts with an upper case.\n",
    "old_exp = '(?:[\\s]|^)[Jj][Aa][Nn][\\.]'\n",
    "new_exp = ' Jan '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ff][Ee][Bb][\\.]'\n",
    "new_exp = ' Feb '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Mm][Aa][Rr][\\.]'\n",
    "new_exp = ' Mar '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Pp][Rr][\\.]'\n",
    "new_exp = ' Apr '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Jj][Uu][Nn][\\.]'\n",
    "new_exp = ' Jun '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Jj][Uu][Ll][\\.]'\n",
    "new_exp = ' Jul '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Aa][Uu][Gg][\\.]'\n",
    "new_exp = ' Aug '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Pp][\\.]'\n",
    "new_exp = ' Sep '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Oo][Cc][Tt][\\.]'\n",
    "new_exp = ' Oct '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Nn][Oo][Vv][\\.]'\n",
    "new_exp = ' Nov '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Dd][Ee][Cc][\\.]'\n",
    "new_exp = ' Dec '\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40288038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# / : a space\n",
    "# $ : **remove** (not care about a few usages for slang)\n",
    "# % : **replace to \" percent \"**\n",
    "# # : **replace to an underbar**  (to tag proper noun, words are either special noun or number)\n",
    "# & : **replace to \"and\"** if spaced, **replace to an underbar otherwise\"\n",
    "\n",
    "# [] () {} : \\_mytag_parentheses_\n",
    "\n",
    "\n",
    "old_exp = '[\\/]'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\$]'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\%]'\n",
    "new_exp = ' percent '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\#]'\n",
    "new_exp = '_'\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\s][\\&][\\s]'\n",
    "new_exp = ' and '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\&]'\n",
    "new_exp = '_'\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\[\\{\\(][\\s]?[\\w]+[\\s]?[\\]\\}\\)]'\n",
    "new_exp = ' _mytag_parentheses_ '\n",
    "replace_regex(old_exp,new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6fed019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dot dot dot\n",
    "old_exp = '[\\.][\\.]+'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e159668",
   "metadata": {},
   "source": [
    "## 2. Word tokenize and PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75cebbbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Abbreviation: AB, A.B, A.B. -> _a_b_ already done for frequent words\n",
    "\n",
    "def tokenizer(corpus, verbose=False):\n",
    "\n",
    "    tb_tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    words = tb_tokenizer.tokenize(corpus)\n",
    "    \n",
    "    pos = pos_tag(words)\n",
    "        \n",
    "    return pos\n",
    "    \n",
    "\n",
    "df0['pos'] = df0.title.apply(tokenizer)\n",
    "df1['pos'] = df1.title.apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85f2dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayse model\n",
    "\n",
    "c = df1.loc[10]\n",
    "\n",
    "#print(c.title,'\\n', c.pos)\n",
    "\n",
    "def convert_pos(pos_only):\n",
    "    \n",
    "    #word_pos = pos_tag([word])\n",
    "\n",
    "    tag = ''\n",
    "    try:\n",
    "        tag = pos_only[:2]\n",
    "    except:\n",
    "        tag = 'n'\n",
    "    \n",
    "    if tag == 'JJ':\n",
    "        tag = 'a'\n",
    "    elif tag == 'NN':\n",
    "        tag = 'n'\n",
    "    elif tag == 'RB':\n",
    "        tag = 'r'\n",
    "    elif tag == 'VB':\n",
    "        tag = 'v'\n",
    "    else:\n",
    "        tag = 'n'\n",
    "        \n",
    "    return tag\n",
    "\n",
    "def gen_organized_column(pos_tag_series):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rgx = re.compile('[\\w]+[\\'\\w+]?|[\\:\\;\\!\\?\\.]')\n",
    "    \n",
    "    col_words = []\n",
    "    col_minimal_words = []\n",
    "    col_pos = []\n",
    "\n",
    "    for pos_tag_row in pos_tag_series:\n",
    "        \n",
    "        words_list = []\n",
    "        minimal_words_list = []\n",
    "        pos_list = []\n",
    "\n",
    "        for pair in pos_tag_row:\n",
    "    \n",
    "            token_list = rgx.findall(pair[0].lower())\n",
    "            pos = pair[1]\n",
    "    \n",
    "            # skip special character token\n",
    "            if not bool(token_list):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            token = ' '.join(token_list)\n",
    "            \n",
    "            #print(token,pos)\n",
    "\n",
    "            words_list.append(token)\n",
    "            pos_list.append(pos)\n",
    "            \n",
    "            \n",
    "            # Minimal words\n",
    "            token_list = [x for x in token_list if not x in stop_words and len(x)>2]\n",
    "            \n",
    "            \n",
    "            if bool(token_list):\n",
    "                token = ' '.join(token_list)\n",
    "                token = token.lower()\n",
    "                minimal_words_list.append(lemmatizer.lemmatize(token, convert_pos(pos)))\n",
    "        \n",
    "        words_list = ' '.join(words_list)\n",
    "        minimal_words_list = ' '.join(minimal_words_list)\n",
    "        pos_list = ' '.join(pos_list)\n",
    "        \n",
    "        col_words.append(words_list)\n",
    "        col_minimal_words.append(minimal_words_list)\n",
    "        col_pos.append(pos_list)\n",
    "        \n",
    "    if not len(pos_tag_series)==len(col_words) or \\\n",
    "        not len(pos_tag_series)==len(col_minimal_words) or \\\n",
    "        not len(pos_tag_series)==len(col_pos):\n",
    "        \n",
    "        return 'Error: array length does not match'\n",
    "    else:\n",
    "        return  pd.Series(col_words, index=pos_tag_series.index), \\\n",
    "                pd.Series(col_pos, index=pos_tag_series.index), \\\n",
    "                pd.Series(col_minimal_words, index=pos_tag_series.index)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a952ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_words0, col_pos0, col_minimal_words0 = gen_organized_column(df0.pos)\n",
    "col_words1, col_pos1, col_minimal_words1 = gen_organized_column(df1.pos)\n",
    "\n",
    "df0['cleaned_words'] = col_words0\n",
    "df0['cleaned_pos'] = col_pos0\n",
    "df0['minimal_words'] = col_minimal_words0\n",
    "\n",
    "df1['cleaned_words'] = col_words1\n",
    "df1['cleaned_pos'] = col_pos1\n",
    "df1['minimal_words'] = col_minimal_words1\n",
    "\n",
    "df0['org_title'] = df0_org.title\n",
    "df1['org_title'] = df1_org.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20edc86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pos</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>cleaned_pos</th>\n",
       "      <th>minimal_words</th>\n",
       "      <th>org_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>Trump picks Boeing executive Shanahan to become Pentagon's No. 2</td>\n",
       "      <td>[(Trump, NN), (picks, NNS), (Boeing, NNP), (executive, NN), (Shanahan, NNP), (to, TO), (become, VB), (Pentagon, NNP), ('s, POS), (No., NN), (2, CD)]</td>\n",
       "      <td>trump picks boeing executive shanahan to become pentagon s no . 2</td>\n",
       "      <td>NN NNS NNP NN NNP TO VB NNP POS NN CD</td>\n",
       "      <td>trump pick boeing executive shanahan become pentagon</td>\n",
       "      <td>Trump picks Boeing executive Shanahan to become Pentagon's No.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>China says it wants smooth military ties with Trump</td>\n",
       "      <td>[(China, NNP), (says, VBZ), (it, PRP), (wants, VBZ), (smooth, JJ), (military, JJ), (ties, NNS), (with, IN), (Trump, NNP)]</td>\n",
       "      <td>china says it wants smooth military ties with trump</td>\n",
       "      <td>NNP VBZ PRP VBZ JJ JJ NNS IN NNP</td>\n",
       "      <td>china say want smooth military tie trump</td>\n",
       "      <td>China says it wants smooth military ties with Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>Trump to nominate Richard Spencer for Navy secretary: White House</td>\n",
       "      <td>[(Trump, NN), (to, TO), (nominate, VB), (Richard, NNP), (Spencer, NNP), (for, IN), (Navy, NNP), (secretary, NN), (:, :), (White, NNP), (House, NNP)]</td>\n",
       "      <td>trump to nominate richard spencer for navy secretary : white house</td>\n",
       "      <td>NN TO VB NNP NNP IN NNP NN : NNP NNP</td>\n",
       "      <td>trump nominate richard spencer navy secretary white house</td>\n",
       "      <td>Trump to nominate Richard Spencer for Navy secretary: White House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13581</th>\n",
       "      <td>Danish divers find arm assumed to be that of dismembered journalist: police</td>\n",
       "      <td>[(Danish, JJ), (divers, NNS), (find, VBP), (arm, NN), (assumed, VBN), (to, TO), (be, VB), (that, IN), (of, IN), (dismembered, JJ), (journalist, NN), (:, :), (police, NN)]</td>\n",
       "      <td>danish divers find arm assumed to be that of dismembered journalist : police</td>\n",
       "      <td>JJ NNS VBP NN VBN TO VB IN IN JJ NN : NN</td>\n",
       "      <td>danish diver find arm assume dismembered journalist police</td>\n",
       "      <td>Danish divers find arm assumed to be that of dismembered journalist: police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>Do not vote for 'demagogue' Trump, U.S. Today tells its readers</td>\n",
       "      <td>[(Do, VB), (not, RB), (vote, VB), (for, IN), ('demagogue, NNP), (', POS), (Trump, NNP), (,, ,), (U.S., NNP), (Today, NNP), (tells, VBZ), (its, PRP$), (readers, NNS)]</td>\n",
       "      <td>do not vote for demagogue trump u . s . today tells its readers</td>\n",
       "      <td>VB RB VB IN NNP NNP NNP NNP VBZ PRP$ NNS</td>\n",
       "      <td>vote demagogue trump today tell reader</td>\n",
       "      <td>Do not vote for 'demagogue' Trump, USA Today tells its readers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20335</th>\n",
       "      <td>Austrian president to insist on pro-EU government after election</td>\n",
       "      <td>[(Austrian, JJ), (president, NN), (to, TO), (insist, VB), (on, IN), (pro-EU, JJ), (government, NN), (after, IN), (election, NN)]</td>\n",
       "      <td>austrian president to insist on pro eu government after election</td>\n",
       "      <td>JJ NN TO VB IN JJ NN IN NN</td>\n",
       "      <td>austrian president insist pro government election</td>\n",
       "      <td>Austrian president to insist on pro-EU government after election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>Big security risks in Trump feud with spy agencies, officials say</td>\n",
       "      <td>[(Big, JJ), (security, NN), (risks, NNS), (in, IN), (Trump, NNP), (feud, NN), (with, IN), (spy, NN), (agencies, NNS), (,, ,), (officials, NNS), (say, VBP)]</td>\n",
       "      <td>big security risks in trump feud with spy agencies officials say</td>\n",
       "      <td>JJ NN NNS IN NNP NN IN NN NNS NNS VBP</td>\n",
       "      <td>big security risk trump feud spy agency official say</td>\n",
       "      <td>Big security risks in Trump feud with spy agencies, officials say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17755</th>\n",
       "      <td>Britain's Queen Elizabeth bows out of Remembrance wreath-laying ceremony</td>\n",
       "      <td>[(Britain, NNP), ('s, POS), (Queen, NNP), (Elizabeth, NNP), (bows, VBZ), (out, IN), (of, IN), (Remembrance, NNP), (wreath-laying, NN), (ceremony, NN)]</td>\n",
       "      <td>britain s queen elizabeth bows out of remembrance wreath laying ceremony</td>\n",
       "      <td>NNP POS NNP NNP VBZ IN IN NNP NN NN</td>\n",
       "      <td>britain queen elizabeth bow remembrance wreath laying ceremony</td>\n",
       "      <td>Britain's Queen Elizabeth bows out of Remembrance wreath-laying ceremony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>Soccer star Weah leads most counties in Liberia presidential election vote</td>\n",
       "      <td>[(Soccer, NNP), (star, NN), (Weah, NNP), (leads, VBZ), (most, JJS), (counties, NNS), (in, IN), (Liberia, NNP), (presidential, JJ), (election, NN), (vote, NN)]</td>\n",
       "      <td>soccer star weah leads most counties in liberia presidential election vote</td>\n",
       "      <td>NNP NN NNP VBZ JJS NNS IN NNP JJ NN NN</td>\n",
       "      <td>soccer star weah lead county liberia presidential election vote</td>\n",
       "      <td>Soccer star Weah leads most counties in Liberia presidential election vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>Peru's Fujimori asks for forgiveness, thanks Kuczynski for pardon</td>\n",
       "      <td>[(Peru, NNP), ('s, POS), (Fujimori, NNP), (asks, VBZ), (for, IN), (forgiveness, NN), (,, ,), (thanks, NNS), (Kuczynski, NNP), (for, IN), (pardon, NN)]</td>\n",
       "      <td>peru s fujimori asks for forgiveness thanks kuczynski for pardon</td>\n",
       "      <td>NNP POS NNP VBZ IN NN NNS NNP IN NN</td>\n",
       "      <td>peru fujimori ask forgiveness thanks kuczynski pardon</td>\n",
       "      <td>Peru's Fujimori asks for forgiveness, thanks Kuczynski for pardon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             title  \\\n",
       "4919              Trump picks Boeing executive Shanahan to become Pentagon's No. 2   \n",
       "7000                           China says it wants smooth military ties with Trump   \n",
       "3415             Trump to nominate Richard Spencer for Navy secretary: White House   \n",
       "13581  Danish divers find arm assumed to be that of dismembered journalist: police   \n",
       "7964               Do not vote for 'demagogue' Trump, U.S. Today tells its readers   \n",
       "20335             Austrian president to insist on pro-EU government after election   \n",
       "6324             Big security risks in Trump feud with spy agencies, officials say   \n",
       "17755     Britain's Queen Elizabeth bows out of Remembrance wreath-laying ceremony   \n",
       "17622   Soccer star Weah leads most counties in Liberia presidential election vote   \n",
       "11300            Peru's Fujimori asks for forgiveness, thanks Kuczynski for pardon   \n",
       "\n",
       "                                                                                                                                                                              pos  \\\n",
       "4919                         [(Trump, NN), (picks, NNS), (Boeing, NNP), (executive, NN), (Shanahan, NNP), (to, TO), (become, VB), (Pentagon, NNP), ('s, POS), (No., NN), (2, CD)]   \n",
       "7000                                                    [(China, NNP), (says, VBZ), (it, PRP), (wants, VBZ), (smooth, JJ), (military, JJ), (ties, NNS), (with, IN), (Trump, NNP)]   \n",
       "3415                         [(Trump, NN), (to, TO), (nominate, VB), (Richard, NNP), (Spencer, NNP), (for, IN), (Navy, NNP), (secretary, NN), (:, :), (White, NNP), (House, NNP)]   \n",
       "13581  [(Danish, JJ), (divers, NNS), (find, VBP), (arm, NN), (assumed, VBN), (to, TO), (be, VB), (that, IN), (of, IN), (dismembered, JJ), (journalist, NN), (:, :), (police, NN)]   \n",
       "7964        [(Do, VB), (not, RB), (vote, VB), (for, IN), ('demagogue, NNP), (', POS), (Trump, NNP), (,, ,), (U.S., NNP), (Today, NNP), (tells, VBZ), (its, PRP$), (readers, NNS)]   \n",
       "20335                                            [(Austrian, JJ), (president, NN), (to, TO), (insist, VB), (on, IN), (pro-EU, JJ), (government, NN), (after, IN), (election, NN)]   \n",
       "6324                  [(Big, JJ), (security, NN), (risks, NNS), (in, IN), (Trump, NNP), (feud, NN), (with, IN), (spy, NN), (agencies, NNS), (,, ,), (officials, NNS), (say, VBP)]   \n",
       "17755                      [(Britain, NNP), ('s, POS), (Queen, NNP), (Elizabeth, NNP), (bows, VBZ), (out, IN), (of, IN), (Remembrance, NNP), (wreath-laying, NN), (ceremony, NN)]   \n",
       "17622              [(Soccer, NNP), (star, NN), (Weah, NNP), (leads, VBZ), (most, JJS), (counties, NNS), (in, IN), (Liberia, NNP), (presidential, JJ), (election, NN), (vote, NN)]   \n",
       "11300                      [(Peru, NNP), ('s, POS), (Fujimori, NNP), (asks, VBZ), (for, IN), (forgiveness, NN), (,, ,), (thanks, NNS), (Kuczynski, NNP), (for, IN), (pardon, NN)]   \n",
       "\n",
       "                                                                      cleaned_words  \\\n",
       "4919              trump picks boeing executive shanahan to become pentagon s no . 2   \n",
       "7000                            china says it wants smooth military ties with trump   \n",
       "3415             trump to nominate richard spencer for navy secretary : white house   \n",
       "13581  danish divers find arm assumed to be that of dismembered journalist : police   \n",
       "7964                do not vote for demagogue trump u . s . today tells its readers   \n",
       "20335              austrian president to insist on pro eu government after election   \n",
       "6324               big security risks in trump feud with spy agencies officials say   \n",
       "17755      britain s queen elizabeth bows out of remembrance wreath laying ceremony   \n",
       "17622    soccer star weah leads most counties in liberia presidential election vote   \n",
       "11300              peru s fujimori asks for forgiveness thanks kuczynski for pardon   \n",
       "\n",
       "                                    cleaned_pos  \\\n",
       "4919      NN NNS NNP NN NNP TO VB NNP POS NN CD   \n",
       "7000           NNP VBZ PRP VBZ JJ JJ NNS IN NNP   \n",
       "3415       NN TO VB NNP NNP IN NNP NN : NNP NNP   \n",
       "13581  JJ NNS VBP NN VBN TO VB IN IN JJ NN : NN   \n",
       "7964   VB RB VB IN NNP NNP NNP NNP VBZ PRP$ NNS   \n",
       "20335                JJ NN TO VB IN JJ NN IN NN   \n",
       "6324      JJ NN NNS IN NNP NN IN NN NNS NNS VBP   \n",
       "17755       NNP POS NNP NNP VBZ IN IN NNP NN NN   \n",
       "17622    NNP NN NNP VBZ JJS NNS IN NNP JJ NN NN   \n",
       "11300       NNP POS NNP VBZ IN NN NNS NNP IN NN   \n",
       "\n",
       "                                                         minimal_words  \\\n",
       "4919              trump pick boeing executive shanahan become pentagon   \n",
       "7000                          china say want smooth military tie trump   \n",
       "3415         trump nominate richard spencer navy secretary white house   \n",
       "13581       danish diver find arm assume dismembered journalist police   \n",
       "7964                            vote demagogue trump today tell reader   \n",
       "20335                austrian president insist pro government election   \n",
       "6324              big security risk trump feud spy agency official say   \n",
       "17755   britain queen elizabeth bow remembrance wreath laying ceremony   \n",
       "17622  soccer star weah lead county liberia presidential election vote   \n",
       "11300            peru fujimori ask forgiveness thanks kuczynski pardon   \n",
       "\n",
       "                                                                         org_title  \n",
       "4919               Trump picks Boeing executive Shanahan to become Pentagon's No.2  \n",
       "7000                           China says it wants smooth military ties with Trump  \n",
       "3415             Trump to nominate Richard Spencer for Navy secretary: White House  \n",
       "13581  Danish divers find arm assumed to be that of dismembered journalist: police  \n",
       "7964                Do not vote for 'demagogue' Trump, USA Today tells its readers  \n",
       "20335             Austrian president to insist on pro-EU government after election  \n",
       "6324             Big security risks in Trump feud with spy agencies, officials say  \n",
       "17755     Britain's Queen Elizabeth bows out of Remembrance wreath-laying ceremony  \n",
       "17622   Soccer star Weah leads most counties in Liberia presidential election vote  \n",
       "11300            Peru's Fujimori asks for forgiveness, thanks Kuczynski for pardon  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pos</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>cleaned_pos</th>\n",
       "      <th>minimal_words</th>\n",
       "      <th>org_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>WATCH: This Video Of The GOP’S Convention Horror Show Should Terrify Everyone Who Sees It</td>\n",
       "      <td>[(WATCH, NN), (:, :), (This, DT), (Video, NNP), (Of, IN), (The, DT), (GOP’S, NNP), (Convention, NNP), (Horror, NNP), (Show, NNP), (Should, NNP), (Terrify, NNP), (Everyone, NNP), (Who, WP), (Sees, ...</td>\n",
       "      <td>watch : this video of the gop s convention horror show should terrify everyone who sees it</td>\n",
       "      <td>NN : DT NNP IN DT NNP NNP NNP NNP NNP NNP NNP WP VBZ PRP</td>\n",
       "      <td>watch video gop convention horror show terrify everyone see</td>\n",
       "      <td>WATCH: This Video Of The GOP’S Convention Horror Show Should Terrify Everyone Who Sees It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>STATE DEPARTMENT OFFICIALS OUT! Connected To Benghazi Scandal And Clinton E-mail Scandal  _mytag_parentheses_</td>\n",
       "      <td>[(STATE, NNP), (DEPARTMENT, NNP), (OFFICIALS, NNP), (OUT, NNP), (!, .), (Connected, VBN), (To, TO), (Benghazi, NNP), (Scandal, NNP), (And, CC), (Clinton, NNP), (E-mail, NNP), (Scandal, NNP), (_myt...</td>\n",
       "      <td>state department officials out ! connected to benghazi scandal and clinton e mail scandal _mytag_parentheses_</td>\n",
       "      <td>NNP NNP NNP NNP . VBN TO NNP NNP CC NNP NNP NNP NN</td>\n",
       "      <td>state department official connect benghazi scandal clinton mail scandal _mytag_parentheses_</td>\n",
       "      <td>STATE DEPARTMENT OFFICIALS OUT! Connected To Benghazi Scandal And Clinton E-mail Scandal [Video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21122</th>\n",
       "      <td>THIS INTERNATIONAL COMPANY Is Luring Refugees And Illegals To America…Do You Buy Meat From Them?</td>\n",
       "      <td>[(THIS, NNP), (INTERNATIONAL, NNP), (COMPANY, NNP), (Is, VBZ), (Luring, VBG), (Refugees, NNP), (And, CC), (Illegals, NNP), (To, TO), (America…Do, NNP), (You, PRP), (Buy, VBP), (Meat, NNP), (From, ...</td>\n",
       "      <td>this international company is luring refugees and illegals to america do you buy meat from them ?</td>\n",
       "      <td>NNP NNP NNP VBZ VBG NNP CC NNP TO NNP PRP VBP NNP IN NNP .</td>\n",
       "      <td>international company lure refugee illegals america buy meat</td>\n",
       "      <td>THIS INTERNATIONAL COMPANY Is Luring Refugees And Illegals To America…Do You Buy Meat From Them?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21317</th>\n",
       "      <td>Why Do Hillary And Barack Choose Islam Over Christianity Every Time?</td>\n",
       "      <td>[(Why, WRB), (Do, VBP), (Hillary, NNP), (And, CC), (Barack, NNP), (Choose, NNP), (Islam, NNP), (Over, NNP), (Christianity, NNP), (Every, NNP), (Time, NNP), (?, .)]</td>\n",
       "      <td>why do hillary and barack choose islam over christianity every time ?</td>\n",
       "      <td>WRB VBP NNP CC NNP NNP NNP NNP NNP NNP NNP .</td>\n",
       "      <td>hillary barack choose islam christianity every time</td>\n",
       "      <td>Why Do Hillary And Barack Choose Islam Over Christianity Every Time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22987</th>\n",
       "      <td>Episode _174 – SUNDAY WIRE: ‘Fake News’ Week In Review</td>\n",
       "      <td>[(Episode, NNP), (_174, NNP), (–, NNP), (SUNDAY, NNP), (WIRE, NNP), (:, :), (‘Fake, NN), (News’, NNP), (Week, NNP), (In, IN), (Review, NNP)]</td>\n",
       "      <td>episode _174 sunday wire : fake news week in review</td>\n",
       "      <td>NNP NNP NNP NNP : NN NNP NNP IN NNP</td>\n",
       "      <td>episode _174 sunday wire fake news week review</td>\n",
       "      <td>Episode #174 – SUNDAY WIRE: ‘Fake News’ Week In Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14857</th>\n",
       "      <td>Obama’s Gas-Guzzling Motorcade To Paris Climate Talks Had A Huge Price Tag For The American Taxpayer</td>\n",
       "      <td>[(Obama’s, NNP), (Gas-Guzzling, NNP), (Motorcade, NNP), (To, TO), (Paris, NNP), (Climate, NNP), (Talks, NNP), (Had, VBD), (A, DT), (Huge, NNP), (Price, NNP), (Tag, NNP), (For, IN), (The, DT), (Ame...</td>\n",
       "      <td>obama s gas guzzling motorcade to paris climate talks had a huge price tag for the american taxpayer</td>\n",
       "      <td>NNP NNP NNP TO NNP NNP NNP VBD DT NNP NNP NNP IN DT NNP NNP</td>\n",
       "      <td>obama gas guzzling motorcade paris climate talk huge price tag american taxpayer</td>\n",
       "      <td>Obama’s Gas-Guzzling Motorcade To Paris Climate Talks Had A Huge Price Tag For The American Taxpayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14800</th>\n",
       "      <td>JUDGE JEANINE HAMMERS OBAMA: “There’s a hero in Washington and you need to let him do his job!”  _mytag_parentheses_</td>\n",
       "      <td>[(JUDGE, NNP), (JEANINE, NNP), (HAMMERS, NNP), (OBAMA, NNP), (:, :), (“There’s, VB), (a, DT), (hero, NN), (in, IN), (Washington, NNP), (and, CC), (you, PRP), (need, VBP), (to, TO), (let, VB), (him...</td>\n",
       "      <td>judge jeanine hammers obama : there s a hero in washington and you need to let him do his job ! _mytag_parentheses_</td>\n",
       "      <td>NNP NNP NNP NNP : VB DT NN IN NNP CC PRP VBP TO VB PRP VB PRP$ NN . NN</td>\n",
       "      <td>judge jeanine hammer obama hero washington need let job _mytag_parentheses_</td>\n",
       "      <td>JUDGE JEANINE HAMMERS OBAMA: “There’s a hero in Washington and you need to let him do his job!” [Video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>Advisers Leak What Trump Does When He Sees Russia Scandal On TV — It’s Really, Really Weird</td>\n",
       "      <td>[(Advisers, NNS), (Leak, VBP), (What, WP), (Trump, NNP), (Does, VBZ), (When, WRB), (He, PRP), (Sees, VBZ), (Russia, NNP), (Scandal, NNP), (On, IN), (TV, NN), (—, NNP), (It’s, NNP), (Really, NNP), ...</td>\n",
       "      <td>advisers leak what trump does when he sees russia scandal on tv it s really really weird</td>\n",
       "      <td>NNS VBP WP NNP VBZ WRB PRP VBZ NNP NNP IN NN NNP NNP NNP NNP</td>\n",
       "      <td>adviser leak trump see russia scandal really really weird</td>\n",
       "      <td>Advisers Leak What Trump Does When He Sees Russia Scandal On TV — It’s Really, Really Weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6713</th>\n",
       "      <td>Bigot Cashier LOSES IT On Gay Customer, Refuses Service  _mytag_parentheses_</td>\n",
       "      <td>[(Bigot, NNP), (Cashier, NNP), (LOSES, NNP), (IT, NNP), (On, IN), (Gay, NNP), (Customer, NNP), (,, ,), (Refuses, NNP), (Service, NNP), (_mytag_parentheses_, VBD)]</td>\n",
       "      <td>bigot cashier loses it on gay customer refuses service _mytag_parentheses_</td>\n",
       "      <td>NNP NNP NNP NNP IN NNP NNP NNP NNP VBD</td>\n",
       "      <td>bigot cashier loses gay customer refuse service _mytag_parentheses_</td>\n",
       "      <td>Bigot Cashier LOSES IT On Gay Customer, Refuses Service (VIDEO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21793</th>\n",
       "      <td>BALTIMORE POLICE UNION WANTS AN INDEPENDANT PROSECUTOR: MOSBY HAS CONNECTIONS TO FREDDIE GRAY FAMILY</td>\n",
       "      <td>[(BALTIMORE, NNP), (POLICE, NNP), (UNION, NNP), (WANTS, NNP), (AN, NNP), (INDEPENDANT, NNP), (PROSECUTOR, NNP), (:, :), (MOSBY, NNP), (HAS, NNP), (CONNECTIONS, NNP), (TO, NNP), (FREDDIE, NNP), (GR...</td>\n",
       "      <td>baltimore police union wants an independant prosecutor : mosby has connections to freddie gray family</td>\n",
       "      <td>NNP NNP NNP NNP NNP NNP NNP : NNP NNP NNP NNP NNP NNP NNP</td>\n",
       "      <td>baltimore police union want independant prosecutor mosby connection freddie gray family</td>\n",
       "      <td>BALTIMORE POLICE UNION WANTS AN INDEPENDANT PROSECUTOR: MOSBY HAS CONNECTIONS TO FREDDIE GRAY FAMILY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       title  \\\n",
       "5360                               WATCH: This Video Of The GOP’S Convention Horror Show Should Terrify Everyone Who Sees It   \n",
       "11802         STATE DEPARTMENT OFFICIALS OUT! Connected To Benghazi Scandal And Clinton E-mail Scandal  _mytag_parentheses_    \n",
       "21122                       THIS INTERNATIONAL COMPANY Is Luring Refugees And Illegals To America…Do You Buy Meat From Them?   \n",
       "21317                                                   Why Do Hillary And Barack Choose Islam Over Christianity Every Time?   \n",
       "22987                                                                 Episode _174 – SUNDAY WIRE: ‘Fake News’ Week In Review   \n",
       "14857                   Obama’s Gas-Guzzling Motorcade To Paris Climate Talks Had A Huge Price Tag For The American Taxpayer   \n",
       "14800  JUDGE JEANINE HAMMERS OBAMA: “There’s a hero in Washington and you need to let him do his job!”  _mytag_parentheses_    \n",
       "1533                             Advisers Leak What Trump Does When He Sees Russia Scandal On TV — It’s Really, Really Weird   \n",
       "6713                                           Bigot Cashier LOSES IT On Gay Customer, Refuses Service  _mytag_parentheses_    \n",
       "21793                   BALTIMORE POLICE UNION WANTS AN INDEPENDANT PROSECUTOR: MOSBY HAS CONNECTIONS TO FREDDIE GRAY FAMILY   \n",
       "\n",
       "                                                                                                                                                                                                           pos  \\\n",
       "5360   [(WATCH, NN), (:, :), (This, DT), (Video, NNP), (Of, IN), (The, DT), (GOP’S, NNP), (Convention, NNP), (Horror, NNP), (Show, NNP), (Should, NNP), (Terrify, NNP), (Everyone, NNP), (Who, WP), (Sees, ...   \n",
       "11802  [(STATE, NNP), (DEPARTMENT, NNP), (OFFICIALS, NNP), (OUT, NNP), (!, .), (Connected, VBN), (To, TO), (Benghazi, NNP), (Scandal, NNP), (And, CC), (Clinton, NNP), (E-mail, NNP), (Scandal, NNP), (_myt...   \n",
       "21122  [(THIS, NNP), (INTERNATIONAL, NNP), (COMPANY, NNP), (Is, VBZ), (Luring, VBG), (Refugees, NNP), (And, CC), (Illegals, NNP), (To, TO), (America…Do, NNP), (You, PRP), (Buy, VBP), (Meat, NNP), (From, ...   \n",
       "21317                                      [(Why, WRB), (Do, VBP), (Hillary, NNP), (And, CC), (Barack, NNP), (Choose, NNP), (Islam, NNP), (Over, NNP), (Christianity, NNP), (Every, NNP), (Time, NNP), (?, .)]   \n",
       "22987                                                             [(Episode, NNP), (_174, NNP), (–, NNP), (SUNDAY, NNP), (WIRE, NNP), (:, :), (‘Fake, NN), (News’, NNP), (Week, NNP), (In, IN), (Review, NNP)]   \n",
       "14857  [(Obama’s, NNP), (Gas-Guzzling, NNP), (Motorcade, NNP), (To, TO), (Paris, NNP), (Climate, NNP), (Talks, NNP), (Had, VBD), (A, DT), (Huge, NNP), (Price, NNP), (Tag, NNP), (For, IN), (The, DT), (Ame...   \n",
       "14800  [(JUDGE, NNP), (JEANINE, NNP), (HAMMERS, NNP), (OBAMA, NNP), (:, :), (“There’s, VB), (a, DT), (hero, NN), (in, IN), (Washington, NNP), (and, CC), (you, PRP), (need, VBP), (to, TO), (let, VB), (him...   \n",
       "1533   [(Advisers, NNS), (Leak, VBP), (What, WP), (Trump, NNP), (Does, VBZ), (When, WRB), (He, PRP), (Sees, VBZ), (Russia, NNP), (Scandal, NNP), (On, IN), (TV, NN), (—, NNP), (It’s, NNP), (Really, NNP), ...   \n",
       "6713                                        [(Bigot, NNP), (Cashier, NNP), (LOSES, NNP), (IT, NNP), (On, IN), (Gay, NNP), (Customer, NNP), (,, ,), (Refuses, NNP), (Service, NNP), (_mytag_parentheses_, VBD)]   \n",
       "21793  [(BALTIMORE, NNP), (POLICE, NNP), (UNION, NNP), (WANTS, NNP), (AN, NNP), (INDEPENDANT, NNP), (PROSECUTOR, NNP), (:, :), (MOSBY, NNP), (HAS, NNP), (CONNECTIONS, NNP), (TO, NNP), (FREDDIE, NNP), (GR...   \n",
       "\n",
       "                                                                                                             cleaned_words  \\\n",
       "5360                            watch : this video of the gop s convention horror show should terrify everyone who sees it   \n",
       "11802        state department officials out ! connected to benghazi scandal and clinton e mail scandal _mytag_parentheses_   \n",
       "21122                    this international company is luring refugees and illegals to america do you buy meat from them ?   \n",
       "21317                                                why do hillary and barack choose islam over christianity every time ?   \n",
       "22987                                                                  episode _174 sunday wire : fake news week in review   \n",
       "14857                 obama s gas guzzling motorcade to paris climate talks had a huge price tag for the american taxpayer   \n",
       "14800  judge jeanine hammers obama : there s a hero in washington and you need to let him do his job ! _mytag_parentheses_   \n",
       "1533                              advisers leak what trump does when he sees russia scandal on tv it s really really weird   \n",
       "6713                                            bigot cashier loses it on gay customer refuses service _mytag_parentheses_   \n",
       "21793                baltimore police union wants an independant prosecutor : mosby has connections to freddie gray family   \n",
       "\n",
       "                                                                  cleaned_pos  \\\n",
       "5360                 NN : DT NNP IN DT NNP NNP NNP NNP NNP NNP NNP WP VBZ PRP   \n",
       "11802                      NNP NNP NNP NNP . VBN TO NNP NNP CC NNP NNP NNP NN   \n",
       "21122              NNP NNP NNP VBZ VBG NNP CC NNP TO NNP PRP VBP NNP IN NNP .   \n",
       "21317                            WRB VBP NNP CC NNP NNP NNP NNP NNP NNP NNP .   \n",
       "22987                                     NNP NNP NNP NNP : NN NNP NNP IN NNP   \n",
       "14857             NNP NNP NNP TO NNP NNP NNP VBD DT NNP NNP NNP IN DT NNP NNP   \n",
       "14800  NNP NNP NNP NNP : VB DT NN IN NNP CC PRP VBP TO VB PRP VB PRP$ NN . NN   \n",
       "1533             NNS VBP WP NNP VBZ WRB PRP VBZ NNP NNP IN NN NNP NNP NNP NNP   \n",
       "6713                                   NNP NNP NNP NNP IN NNP NNP NNP NNP VBD   \n",
       "21793               NNP NNP NNP NNP NNP NNP NNP : NNP NNP NNP NNP NNP NNP NNP   \n",
       "\n",
       "                                                                                     minimal_words  \\\n",
       "5360                                   watch video gop convention horror show terrify everyone see   \n",
       "11802  state department official connect benghazi scandal clinton mail scandal _mytag_parentheses_   \n",
       "21122                                 international company lure refugee illegals america buy meat   \n",
       "21317                                          hillary barack choose islam christianity every time   \n",
       "22987                                               episode _174 sunday wire fake news week review   \n",
       "14857             obama gas guzzling motorcade paris climate talk huge price tag american taxpayer   \n",
       "14800                  judge jeanine hammer obama hero washington need let job _mytag_parentheses_   \n",
       "1533                                     adviser leak trump see russia scandal really really weird   \n",
       "6713                           bigot cashier loses gay customer refuse service _mytag_parentheses_   \n",
       "21793      baltimore police union want independant prosecutor mosby connection freddie gray family   \n",
       "\n",
       "                                                                                                     org_title  \n",
       "5360                 WATCH: This Video Of The GOP’S Convention Horror Show Should Terrify Everyone Who Sees It  \n",
       "11802         STATE DEPARTMENT OFFICIALS OUT! Connected To Benghazi Scandal And Clinton E-mail Scandal [Video]  \n",
       "21122         THIS INTERNATIONAL COMPANY Is Luring Refugees And Illegals To America…Do You Buy Meat From Them?  \n",
       "21317                                     Why Do Hillary And Barack Choose Islam Over Christianity Every Time?  \n",
       "22987                                                   Episode #174 – SUNDAY WIRE: ‘Fake News’ Week In Review  \n",
       "14857     Obama’s Gas-Guzzling Motorcade To Paris Climate Talks Had A Huge Price Tag For The American Taxpayer  \n",
       "14800  JUDGE JEANINE HAMMERS OBAMA: “There’s a hero in Washington and you need to let him do his job!” [Video]  \n",
       "1533               Advisers Leak What Trump Does When He Sees Russia Scandal On TV — It’s Really, Really Weird  \n",
       "6713                                           Bigot Cashier LOSES IT On Gay Customer, Refuses Service (VIDEO)  \n",
       "21793     BALTIMORE POLICE UNION WANTS AN INDEPENDANT PROSECUTOR: MOSBY HAS CONNECTIONS TO FREDDIE GRAY FAMILY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df0.sample(10))\n",
    "display(df1.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb462702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_csv('data/TrueOrganized.csv')\n",
    "df1.to_csv('data/FakeOrganized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4599ef",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We finished cleaning and organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7963e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
