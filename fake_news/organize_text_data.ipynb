{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974bc7b5",
   "metadata": {},
   "source": [
    "# Organize with EDA\n",
    "I'm taking a look only for title, but eventually, both title and text will be used.\n",
    "- For future: mind dots (handle dots without hurting sentence tokenization) in text\n",
    "- Ignore comment about text for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fea3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freq_utils import *\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.help import upenn_tagset\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b49ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_org = pd.read_csv('True.csv')\n",
    "df1_org = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260fa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data we don't use (from eda_raw.ipynb)\n",
    "df0_org.drop(['text','subject','date'], axis=1, inplace=True)\n",
    "df1_org.drop(['text','subject','date'], axis=1, inplace=True)\n",
    "\n",
    "df0_org.drop_duplicates()\n",
    "df1_org.drop_duplicates()\n",
    "\n",
    "df1_org = df1_org[df1_org.title.str.split().str.len()>2]\n",
    "#df0 = df0[df0.text.str.split().str.len()>19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c7560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare modification result\n",
    "df0 = df0_org\n",
    "df1 = df1_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef42714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences_with_this_string(this_string, column_to_look, df_list, df_names, \n",
    "                                     print_words=False, print_set=False, sent_token=False):\n",
    "    \n",
    "    n_dataFrame = len(df_list)\n",
    "    \n",
    "    pat = re.compile(this_string)\n",
    "    \n",
    "    set_list = []\n",
    "    \n",
    "    for i in range(n_dataFrame):\n",
    "        df = df_list[i]\n",
    "        df = df[df[column_to_look].str.contains(this_string, regex= True, na=False)]\n",
    "        \n",
    "        count = df[column_to_look].count()\n",
    "        \n",
    "        \n",
    "        print(this_string,'in',column_to_look,'\\n',df_names[i],':',count)\n",
    "        \n",
    "        if count==0:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if print_set:\n",
    "            df = df.sample(min(len(df),1000), random_state=20)\n",
    "        else:\n",
    "            df = df.sample(min(len(df),20), random_state=20)\n",
    "        \n",
    "        corpus_list = df[column_to_look].to_numpy()\n",
    "        index_list  = df.index.to_numpy()\n",
    "                \n",
    "        example_df = pd.DataFrame(columns=['index','selected_text','selected_words'])\n",
    "        \n",
    "      \n",
    "        for row in range(len(index_list)):\n",
    "            \n",
    "            if sent_token:\n",
    "                sentences = sent_tokenize(corpus_list[row])\n",
    "            \n",
    "                display_text = ''\n",
    "                display_word = []\n",
    "            \n",
    "                for sentence in sentences:\n",
    "                \n",
    "                    if pat.search(sentence):\n",
    "                        display_text += sentence+' '\n",
    "                        display_word += pat.findall(sentence)\n",
    "                                        \n",
    "                example_df.loc[row] = [index_list[row],display_text,display_word]\n",
    "            else:\n",
    "                if pat.search(corpus_list[row]):\n",
    "               \n",
    "                    display_text = corpus_list[row]\n",
    "                    display_word = pat.findall(display_text)\n",
    "                    example_df.loc[row] = [index_list[row],display_text,display_word]\n",
    "                \n",
    "            \n",
    "            \n",
    "        example_df.set_index('index')\n",
    "        \n",
    "        if print_set:    \n",
    "            #word_set = set()\n",
    "            word_set = list()\n",
    "            \n",
    "            lst_list = list(example_df.selected_words)\n",
    "            \n",
    "            for lst in lst_list:\n",
    "                word_set += lst\n",
    "                \n",
    "            #print(word_set)\n",
    "            \n",
    "            word_counter = Counter(word_set)\n",
    "            print(word_counter.most_common(200))\n",
    "        \n",
    "\n",
    "        if not print_words:\n",
    "            example_df.drop(['selected_words'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        display(example_df.sample(min(len(df),20), random_state=20))\n",
    "        \n",
    "        if print_set:\n",
    "            set_list.append(word_set)\n",
    "            \n",
    "    if print_set:\n",
    "        return set_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb7c7",
   "metadata": {},
   "source": [
    "# Digital source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db81896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*[@]+[^\\s]*',        \n",
    "                                 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*//[^\\s]+[.][^\\s]+', \n",
    "                                 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678e876",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- Overall, **fake news quote digital sources much more frequently** than real news. \n",
    "\n",
    "#### @ in title\n",
    "- Real news: only one news has it, when its **topic is about social media account**\n",
    "- Fake news: some are about **social media account**, but some are **slangs** (used like \"*\")\n",
    "\n",
    "#### @ in text\n",
    "- Both real and fake news have @ to **refer social media accounts**.\n",
    "- 20 times **more frequently** used in fake news\n",
    "\n",
    "#### website in text\n",
    "- **Few real news** contains the website address in this dataset.\n",
    "- **A lot of fake news** contains website address. Examples of them were CNN news, Facebook, and YouTube address.\n",
    "\n",
    "### Processing\n",
    "- There are only a few rows, so let's simply change **@ to _**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65840a44",
   "metadata": {},
   "source": [
    "### Replace them to tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c767ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.replace(to_replace='@', value='_mytag_at_', regex=True, inplace=False)\n",
    "df1 = df1.replace(to_replace='@', value='_mytag_at_', regex=True, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a3ed1",
   "metadata": {},
   "source": [
    "# Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37712e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "\n",
    "print_sentences_with_this_string('[^\\s]*[\\*]+[^\\s]*', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10355b2",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- No real news has * in titles.\n",
    "- Some **fake news** have * in **title** to display **slangs**.\n",
    "- Both real and fake news have **\\* in texts**, 14 times **frequently occur in fake news**.\n",
    "- When * is used **in the text**, it is **not always for slangs** (e.g. to emphasize). \n",
    "- It is not **hard to separate** usage of star between **slang and highlighting** based on text pattern.\n",
    "- I'll mark both of those works as `_mytag_slang_` since they have a common meaning and function, **highlighting, anyway.\n",
    "\n",
    "### Processing\n",
    "- Tag words contain * as **slang** (only for title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6631cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace works with *\n",
    "df0.replace(to_replace='[^\\s]*[\\*]+[^\\s]*', value='_mytag_slang_', regex=True, inplace=True)\n",
    "df1.replace(to_replace='[^\\s]*[\\*]+[^\\s]*', value='_mytag_slans_', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc4d98",
   "metadata": {},
   "source": [
    "# Other special characters\n",
    "As seen from the slang character tagging, some special character replaces an alphabet character, therefore, blindly removing all special characters might leave some words meaningless.\n",
    "Let's check how the other special characters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd947dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "sc_title = print_sentences_with_this_string('[^\\s\\w]', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a27dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "c0 = Counter(sc_title[0])\n",
    "c1 = Counter(sc_title[1])\n",
    "\n",
    "print(c0)\n",
    "print(c1)\n",
    "\n",
    "print(set([x[0] for x in c0.most_common(5)]))\n",
    "\n",
    "\n",
    "sc_fake_only = set([x[0] for x in c1]) - set([x[0] for x in c0.most_common(5)])\n",
    "\n",
    "\n",
    "print(sc_fake_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c6c4",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Fake news are more noisy having more kind of special characters. Special characters not used in real news might have a special function. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef95291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for x in c0:\n",
    "    \n",
    "    #sc_regex = '[\\\\' + x + ']'\n",
    "    sc_regex = '[^\\s]*[\\\\' + x + '][^\\s]*'\n",
    "\n",
    "    print(x)\n",
    "    print_sentences_with_this_string(sc_regex, 'title', [df0, df1], ['Real','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3741b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for x in sc_fake_only:\n",
    "    \n",
    "    #sc_regex = '[\\\\' + x + ']'\n",
    "    sc_regex = '[^\\s]*[\\\\' + x + '][^\\s]*'\n",
    "\n",
    "    print(x)\n",
    "    print_sentences_with_this_string(sc_regex, 'title', [df1], ['Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ce59b",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Usages\n",
    "- \\# : hashtag, tv show episode, website address \n",
    "- % : percent, not used in real news, interestingly\n",
    "- -- (longer than a hyphen) : hyphen, some slang but ignorable\n",
    "- ! : exclamation, not used in real news, but remove it in case real news happen to have an exclamation mark is classified as fake\n",
    "- [] (): clickbait, emphasis\n",
    "- } : seems a typo of ] in case of title, in text, it looks like a script. Token with } in text better be removed.\n",
    "- & : and or special words (e.g. Q&A, AT&T)\n",
    "- \\$ : dollor or slang\n",
    "- \\/ : 9/11, 24/7, or clickbait (e.g. video/image)\n",
    "\n",
    "### Note for processings\n",
    "\n",
    "- \\/ : **replace to a space*\n",
    "- [] () {} : **remove with enclosed text** to avoid a strong bias of this dataset\n",
    "- : : **replace to a space** if it is between two numbers (time), **replace to ;** otherwise \n",
    "- ;, ... : **replace to \\.**\n",
    "- Abc. (abbreviation has one dot at the end): **remove dot**\n",
    "\n",
    "Tokenize sentence. Then\n",
    "\n",
    "- — : **replace to -** then do the same as below\n",
    "- \\- : **leave it** if it is hyphen (between two words without space), **remove** otherwise\n",
    "- \\$ : **remove** if followed by a number, **replace to \\_** otherwise (slangs)\n",
    "- \\& : **replace to \"and\"** if spaced, **replace to \\_ otherwise\"\n",
    "- \\% : **replace to \" percent \"**\n",
    "\n",
    "- \\# : remove (words are either special noun or number)\n",
    "- !, ?, , : remove\n",
    "- \\\" : remove\n",
    "\n",
    "Tokenize word. Then\n",
    "- \\' : remove\n",
    "- handle abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5362de5",
   "metadata": {},
   "source": [
    "# Capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8772d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print_sentences_with_this_string('[\\s^\\w][A-Z][^\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdbdb1",
   "metadata": {},
   "source": [
    "### Findings\n",
    "As from EDA, words with capicalized first characters are proper nouns. Name entity recognition would recognize some of them (e.g. \"Trump\"), but some woudn't (e.g. \"White\", \"House\"). However, bigram or trigram would catch such case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4abb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "title0 = df0.sample(1000, random_state=9).title.tolist()\n",
    "title1 = df1.sample(1000, random_state=9).title.tolist()\n",
    "\n",
    "# Make a bigram list\n",
    "# Subtract trigram\n",
    "bi_real = ngram_tokenizer(title0, n=2)\n",
    "bi_fake = ngram_tokenizer(title1, n=2)\n",
    "tr_real = ngram_tokenizer(title0, n=3)\n",
    "tr_fake = ngram_tokenizer(title1, n=3)\n",
    "qd_real = ngram_tokenizer(title0, n=4)\n",
    "qd_fake = ngram_tokenizer(title1, n=4)\n",
    "\n",
    "def combine_uppercase_words(lst):\n",
    "\n",
    "    rg = re.compile('[A-Z]')\n",
    "    \n",
    "    co = Counter(lst)  \n",
    "    ngram_lst  = [x[0] for x in list(co.most_common(100))]\n",
    "    \n",
    "    lst_combine=[]\n",
    "\n",
    "    for x in ngram_lst:\n",
    "        # x: bigram or trigram tuple\n",
    "        ngram = len(x)\n",
    "        \n",
    "        if set([bool(rg.search(x[i][0])) for i in range(ngram)])=={True}:\n",
    "            lst_combine.append(x)\n",
    "    \n",
    "    return ngram_lst, lst_combine\n",
    "            \n",
    "print(combine_uppercase_words(bi_real),'\\n')            \n",
    "print(combine_uppercase_words(tr_real),'\\n')            \n",
    "print(combine_uppercase_words(qd_real),'\\n')            \n",
    "print(combine_uppercase_words(bi_fake),'\\n')            \n",
    "print(combine_uppercase_words(tr_fake),'\\n')            \n",
    "print(combine_uppercase_words(qd_fake),'\\n') \n",
    "\n",
    "bi = combine_uppercase_words(bi_real)[1]\n",
    "tr = combine_uppercase_words(tr_real)[1]\n",
    "\n",
    "for t in tr:\n",
    "    for b in bi:\n",
    "        if set(b).issubset(set(t)):\n",
    "            print(b, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc21f91",
   "metadata": {},
   "source": [
    "### Note for preprocessings\n",
    "- Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c775aa",
   "metadata": {},
   "source": [
    "# Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23a53d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# US, USA, UN, UK... -> Add two \"_\" at the end\n",
    "print_sentences_with_this_string('[A-Z][A-Z]+[\\.]?', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd7b6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Abbreviation with a space between, like U. S.?\n",
    "print_sentences_with_this_string('[A-Z][\\w]*[.][\\s][A-Z][\\w]*[.]', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4336b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Abbreviation with lower cases?\n",
    "print_sentences_with_this_string('[a-z][\\w]*[.][a-z][\\w]*[.][^\\s]*', 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d16bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# U.S., Dr.\n",
    "print_sentences_with_this_string('[A-Z][\\w]*[\\.][\\w\\.]*', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2671f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# single character words\n",
    "print_sentences_with_this_string('[\\s]+[A-Z][\\.][\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a83509",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# two character words\n",
    "print_sentences_with_this_string('[\\s]+[A-Z][\\.]?[A-Z][\\.]?[\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16828b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Words have only one dot at the end\n",
    "print_sentences_with_this_string('[A-Z][\\w]+[\\.][\\s]+', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa8048e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Check other dot examples\n",
    "print_sentences_with_this_string('[^\\s]*[\\.][^\\s]*', 'title', \n",
    "                                 [df0,df1], ['True','Fake'], print_words=True, print_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2266261",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- The dot in any abbreviation will interrupt sentence tokenization.\n",
    "    - Abbreviation at the end of a sentence\n",
    "        - 'I'm in the **U.S.** That is a news!' tokenized into **one** sentence.\n",
    "        - 'I'm in the **Dept.** That is a news!' tokenized into **two** sentences.\n",
    "    - Abbreviation in the middle of a sentence\n",
    "        - 'I'm in the **U.S.** now!' tokenized into **one** sentence.\n",
    "        - 'I'm in the **Dept.** now!' tokenized into **two** sentences.\n",
    "\n",
    "- Due to **irregular capitalization/formatting rules in fake news**, distinguish these words relying on text format **without context** seems **impossible**.\n",
    "- We can ignore possible spaces between abbreviation because they didn't happen (e.g. U. S.).\n",
    "- May (month) vs may (modal verb) vs May (name) is hard to distinguish. I'll leave it up to the learning a context.\n",
    "\n",
    "Here is a note about most frequent words.\n",
    "\n",
    "\n",
    "#### Bigram/Trigram\n",
    "- Some words start with upper case should be combined to have meaning (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)\n",
    "- No point to use fake news bi/trigram to replace Capital+Capical words because it is full of noise.\n",
    "- For 'House', 'Speaker' and 'Ryan', 'Ryan' should be separated.\n",
    "- For now, let's leave it up to learning\n",
    "\n",
    "#### Examples of words that should be recognized as a same word\n",
    "- **US, U.S, U.S. or U.S.A.**: variation of the United States, comes from fake news or typo. \n",
    "\n",
    "#### Examples of words that should be recognized as distinct words\n",
    "- **PM (Prime Minister)**, **P.M. (Post Meridiem)**, and **p.m. (post meridiem)**\n",
    "- **IS (Islamic State)** and **is (be verb)**\n",
    "- **No. (number)** and **no (opposite of yes)**\n",
    "- **IT (information technology) ** and **it (pronoun)**\n",
    "\n",
    "#### Single or two characters words\n",
    "- They can be removed by **stop word** removal. \n",
    "- Most of **single letters** for middle name except **N. in North Korea**. It's ok to drop middle names.\n",
    "- For **two characters** words, add extra \\_ at the end in order not to be disappeared.\n",
    "\n",
    "#### Words have only one dot at the end\n",
    "- These words can distort sentence tokenization.\n",
    "- They are followed by a proper noun (**Dr., Sen., Jr.**), which is ok to be **removed**, or name of month (**Jul.**). Either case, **removing dot** would be enough.\n",
    "\n",
    "### Note for preprocessings\n",
    "- Change **N. Korea**, **N.Korea** to **North Korea**\n",
    "- Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong)\n",
    "- Change abbreviations\n",
    "    - U.N. : \\_u_n_\n",
    "    - Rep. : Rep\n",
    "    - Sept. : Sep\n",
    "    - Sen. : Sen\n",
    "    - Gov. : Gov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c497dd",
   "metadata": {},
   "source": [
    "# Preprocessings\n",
    "\n",
    "#### 1. Least interfering preprocessings\n",
    "\n",
    "1. Replace N. Korea, N.Korea to North Korea (frequently occuring topic)\n",
    "2. Remove single letter capical word (e.g. middle name)\n",
    "3. Abbreviation\n",
    "    - Remove a dot at the end of a Month word starts with an upper case. (Jul. -> Jul)\n",
    "    - No. to number\n",
    "    - U.N. : \\_u_n_\n",
    "    - Rep. : Rep\n",
    "    - Sept. : Sep\n",
    "    - Sen. : Sen\n",
    "    - Gov. : Gov\n",
    "    - PM : \\_p_m_\n",
    "    - P.M. (Post Meridiem), and p.m. (post meridiem): \\_mytag_pm_ (same for a.m.)\n",
    "    - US, U.S, U.S. or U.S.A.: \\_u_s_\n",
    "4. Special characters \n",
    "    - / : a space\n",
    "    - [] () {} : \\_mytag_parentheses_\n",
    "    - $ : remove (not care about a few usages for slang)\n",
    "    - & : replace to \"and\" if spaced, replace to an underbar otherwise\n",
    "    - % : replace to \" percent \"\n",
    "    - \\# : replace to an underbar  (words are either special noun or number)\n",
    "5. Special characters (after abbreviation handling)\n",
    "    - ... : a space\n",
    "    - : : ~replace to a space if it is between two numbers (time), replace to . otherwise~ leave it to tokenizer\n",
    "    - ; : ~replace to \\.~ leave it to tokenizer\n",
    "\n",
    "\n",
    "#### ~2. After sentence tokenization~ No sentenct tokenization for title\n",
    "1. ~Bigram: words that have different meaning if used alone: Combine \\<Capital start\\> + \\<Capical start\\> words (e.g. White+House, North+Korea, Puerto+Rico, Hong+Kong) with an underbar, keep capicalization~ leave it to learning\n",
    "        \n",
    "\n",
    "\n",
    "#### 3. After word tokenization\n",
    "1. Special characters\n",
    "    - — (en dash? em dash?): replace to - (hyphen) then do the same as below\n",
    "    - \\- : replace it to an underbar if it is hyphen (between two words without space), remove otherwise\n",
    "    - !, ?, , : remove\n",
    "    - \\\" : remove\n",
    "2. Abbreviation: AB, A.B -> \\_a_b_\n",
    "\n",
    "\n",
    "#### 4. After PoS tagging\n",
    "    - \\' : remove    \n",
    "    - Uncapitalization\n",
    "\n",
    "<Text items >\n",
    "<Remove news id (location, reuters) >\n",
    "<Replace month abbreviations>\n",
    "<july, jul., Jul, Jul., July : \\_mytag_month_july_ >\n",
    "<May: \\_mytag_month_may_ >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1d518",
   "metadata": {},
   "source": [
    "## Least interfering preprocessings\n",
    "Includes handlings for better sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b227d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_regex(old_exp, new_exp='', verbose=True):\n",
    "    \n",
    "    if verbose:\n",
    "        print_sentences_with_this_string(old_exp, 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n",
    "    \n",
    "    if not new_exp=='':\n",
    "        df0.replace(to_replace=old_exp, value=new_exp, regex=True, inplace=True)\n",
    "        df1.replace(to_replace=old_exp, value=new_exp, regex=True, inplace=True)\n",
    "        if verbose:\n",
    "            print(old_exp,'replaced to',new_exp)\n",
    "            print_sentences_with_this_string(new_exp, 'title', [df0,df1], ['True','Fake'], print_words=True, print_set=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede65d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace N. Korea, N.Korea to North Korea\n",
    "old_exp = '(?:[\\s]|^)[N][\\.][\\s]?Korea'\n",
    "new_exp = ' North Korea'\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6b2db47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove single letter capical word (e.g. middle name)\n",
    "old_exp = '(?:[\\s]|^)[A-Z][\\.](?:[\\s]|$)'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0223ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. to No\n",
    "# U.N. : _u_n_\n",
    "# Rep. : Rep\n",
    "# Sept. : Sep\n",
    "# Sen. : Sen\n",
    "# Gov. : Gov\n",
    "# PM : \\_p_m_\n",
    "# **P.M. (Post Meridiem)**, and **p.m. (post meridiem)**: \\_mytag_pm_ (same for a.m.)\n",
    "# **US, U.S, U.S. or U.S.A.**: \\_u_s_\n",
    "\n",
    "      \n",
    "#old_exp = '(?:[\\s]|^)[N][o][\\.]'\n",
    "#new_exp = ' No '\n",
    "old_exp = '(?:[\\s]|^)[N][o][\\.]'\n",
    "new_exp = ' No. '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[U][\\.]?[N][\\.]?(?:[\\s]|$)'\n",
    "new_exp = ' _u_n_ '\n",
    "#new_exp = ' UN '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Rr][Ee][Pp][\\.]'\n",
    "new_exp = ' Rep '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Pp][Tt][\\.]'\n",
    "new_exp = ' Sept '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Nn][\\.]'\n",
    "new_exp = ' Sen '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Gg][Oo][Vv][\\.]'\n",
    "new_exp = ' Gov '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[P][M](?:[\\s]|$)'\n",
    "new_exp = ' _p_m_ '\n",
    "#new_exp = ' PM '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Pp][\\.][Mm][\\.](?:[\\s]|$)'\n",
    "new_exp = ' _mytag_pm_ '\n",
    "#new_exp = ' p.m. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Aa][\\.][Mm][\\.](?:[\\s]|$)'\n",
    "new_exp = ' _mytag_am_ '\n",
    "#new_exp = ' a.m. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[U][\\.]?[S][\\.]?[A]?[\\.]?(?:[\\s]|$)'\n",
    "new_exp = ' _u_s_ '\n",
    "#new_exp = ' U.S. '\n",
    "replace_regex(old_exp,new_exp, verbose=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fffd5571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove a dot at the end of a word starts with an upper case.\n",
    "old_exp = '(?:[\\s]|^)[Jj][Aa][Nn][\\.]'\n",
    "new_exp = ' Jan '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ff][Ee][Bb][\\.]'\n",
    "new_exp = ' Feb '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Mm][Aa][Rr][\\.]'\n",
    "new_exp = ' Mar '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Pp][Rr][\\.]'\n",
    "new_exp = ' Apr '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Jj][Uu][Nn][\\.]'\n",
    "new_exp = ' Jun '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Jj][Uu][Ll][\\.]'\n",
    "new_exp = ' Jul '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Aa][Uu][Gg][\\.]'\n",
    "new_exp = ' Aug '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Ss][Ee][Pp][\\.]'\n",
    "new_exp = ' Sep '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Oo][Cc][Tt][\\.]'\n",
    "new_exp = ' Oct '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Nn][Oo][Vv][\\.]'\n",
    "new_exp = ' Nov '\n",
    "replace_regex(old_exp, new_exp, verbose=False)\n",
    "\n",
    "old_exp = '(?:[\\s]|^)[Dd][Ee][Cc][\\.]'\n",
    "new_exp = ' Dec '\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40288038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# / : a space\n",
    "# $ : **remove** (not care about a few usages for slang)\n",
    "# % : **replace to \" percent \"**\n",
    "# # : **replace to an underbar**  (to tag proper noun, words are either special noun or number)\n",
    "# & : **replace to \"and\"** if spaced, **replace to an underbar otherwise\"\n",
    "\n",
    "# [] () {} : \\_mytag_parentheses_\n",
    "\n",
    "\n",
    "old_exp = '[\\/]'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\$]'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\%]'\n",
    "new_exp = ' percent '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\#]'\n",
    "new_exp = '_'\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\s][\\&][\\s]'\n",
    "new_exp = ' and '\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\&]'\n",
    "new_exp = '_'\n",
    "replace_regex(old_exp,new_exp, verbose=False)\n",
    "\n",
    "old_exp = '[\\[\\{\\(][\\s]?[\\w]+[\\s]?[\\]\\}\\)]'\n",
    "new_exp = ' _mytag_parentheses_ '\n",
    "replace_regex(old_exp,new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6fed019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dot dot dot\n",
    "old_exp = '[\\.][\\.]+'\n",
    "new_exp = ' '\n",
    "replace_regex(old_exp, new_exp, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e159668",
   "metadata": {},
   "source": [
    "## 2. Word tokenize and PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75cebbbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Abbreviation: AB, A.B, A.B. -> _a_b_ already done for frequent words\n",
    "\n",
    "def tokenizer(corpus, verbose=False):\n",
    "\n",
    "    tb_tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    words = tb_tokenizer.tokenize(corpus)\n",
    "    \n",
    "    pos = pos_tag(words)\n",
    "        \n",
    "    return pos\n",
    "    \n",
    "\n",
    "df0['pos'] = df0.title.apply(tokenizer)\n",
    "df1['pos'] = df1.title.apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "85f2dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayse model\n",
    "\n",
    "c = df1.loc[10]\n",
    "\n",
    "#print(c.title,'\\n', c.pos)\n",
    "\n",
    "def convert_pos(pos_only):\n",
    "    \n",
    "    #word_pos = pos_tag([word])\n",
    "\n",
    "    tag = ''\n",
    "    try:\n",
    "        tag = pos_only[:2]\n",
    "    except:\n",
    "        tag = 'n'\n",
    "    \n",
    "    if tag == 'JJ':\n",
    "        tag = 'a'\n",
    "    elif tag == 'NN':\n",
    "        tag = 'n'\n",
    "    elif tag == 'RB':\n",
    "        tag = 'r'\n",
    "    elif tag == 'VB':\n",
    "        tag = 'v'\n",
    "    else:\n",
    "        tag = 'n'\n",
    "        \n",
    "    return tag\n",
    "\n",
    "def gen_organized_column(pos_tag_series):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rgx = re.compile('[\\w]+[\\'\\w+]?|[\\:\\;\\!\\?]')\n",
    "    \n",
    "    col_words = []\n",
    "    col_minimal_words = []\n",
    "    col_pos = []\n",
    "\n",
    "    for pos_tag_row in pos_tag_series:\n",
    "        \n",
    "        words_list = []\n",
    "        minimal_words_list = []\n",
    "        pos_list = []\n",
    "\n",
    "        for pair in pos_tag_row:\n",
    "    \n",
    "            token_list = rgx.findall(pair[0].lower())\n",
    "            pos = pair[1]\n",
    "    \n",
    "            # skip special character token\n",
    "            if not bool(token_list):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            token = ' '.join(token_list)\n",
    "            \n",
    "            #print(token,pos)\n",
    "\n",
    "            words_list.append(token)\n",
    "            pos_list.append(pos)\n",
    "            \n",
    "            \n",
    "            # Minimal words\n",
    "            token_list = [x for x in token_list if not x in stop_words and len(x)>2]\n",
    "            \n",
    "            \n",
    "            if bool(token_list):\n",
    "                token = ' '.join(token_list)\n",
    "                token = token.lower()\n",
    "                minimal_words_list.append(lemmatizer.lemmatize(token, convert_pos(pos)))\n",
    "        \n",
    "        col_words.append(words_list)\n",
    "        col_minimal_words.append(minimal_words_list)\n",
    "        col_pos.append(pos_list)\n",
    "        \n",
    "    if not len(pos_tag_series)==len(col_words) or \\\n",
    "        not len(pos_tag_series)==len(col_minimal_words) or \\\n",
    "        not len(pos_tag_series)==len(col_pos):\n",
    "        \n",
    "        return 'Error: array length does not match'\n",
    "    else:\n",
    "        return  pd.Series(col_words, index=pos_tag_series.index), \\\n",
    "                pd.Series(col_minimal_words, index=pos_tag_series.index), \\\n",
    "                pd.Series(col_pos, index=pos_tag_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a952ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_words0, col_minimal_words0, col_pos0 = gen_organized_column(df0.pos)\n",
    "col_words1, col_minimal_words1, col_pos1 = gen_organized_column(df1.pos)\n",
    "\n",
    "df0['cleaned_words'] = col_words0\n",
    "df0['minimal_words'] = col_minimal_words0\n",
    "df0['cleaned_pos'] = col_pos0\n",
    "\n",
    "df1['cleaned_words'] = col_words1\n",
    "df1['minimal_words'] = col_minimal_words1\n",
    "df1['cleaned_pos'] = col_pos1\n",
    "\n",
    "df0['org_title'] = df0_org.title\n",
    "df1['org_title'] = df1_org.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "20edc86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pos</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>minimal_words</th>\n",
       "      <th>cleaned_pos</th>\n",
       "      <th>org_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16151</th>\n",
       "      <td>Kurdish oil flows to Turkey resume after technical stoppage: shipping source</td>\n",
       "      <td>[(Kurdish, JJ), (oil, NN), (flows, VBZ), (to, TO), (Turkey, NNP), (resume, NN), (after, IN), (technical, JJ), (stoppage, NN), (:, :), (shipping, NN), (source, NN)]</td>\n",
       "      <td>[kurdish, oil, flows, to, turkey, resume, after, technical, stoppage, :, shipping, source]</td>\n",
       "      <td>[kurdish, oil, flow, turkey, resume, technical, stoppage, shipping, source]</td>\n",
       "      <td>[JJ, NN, VBZ, TO, NNP, NN, IN, JJ, NN, :, NN, NN]</td>\n",
       "      <td>Kurdish oil flows to Turkey resume after technical stoppage: shipping source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21001</th>\n",
       "      <td>Venezuelan President Maduro will not go to _u_n_ rights forum</td>\n",
       "      <td>[(Venezuelan, NNP), (President, NNP), (Maduro, NNP), (will, MD), (not, RB), (go, VB), (to, TO), (_u_n_, VB), (rights, NNS), (forum, NN)]</td>\n",
       "      <td>[venezuelan, president, maduro, will, not, go, to, _u_n_, rights, forum]</td>\n",
       "      <td>[venezuelan, president, maduro, _u_n_, right, forum]</td>\n",
       "      <td>[NNP, NNP, NNP, MD, RB, VB, TO, VB, NNS, NN]</td>\n",
       "      <td>Venezuelan President Maduro will not go to U.N. rights forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>Anti-Assad nations say no to Syria reconstruction until political process on track</td>\n",
       "      <td>[(Anti-Assad, JJ), (nations, NNS), (say, VBP), (no, DT), (to, TO), (Syria, NNP), (reconstruction, NN), (until, IN), (political, JJ), (process, NN), (on, IN), (track, NN)]</td>\n",
       "      <td>[anti assad, nations, say, no, to, syria, reconstruction, until, political, process, on, track]</td>\n",
       "      <td>[anti assad, nation, say, syria, reconstruction, political, process, track]</td>\n",
       "      <td>[JJ, NNS, VBP, DT, TO, NNP, NN, IN, JJ, NN, IN, NN]</td>\n",
       "      <td>Anti-Assad nations say no to Syria reconstruction until political process on track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15718</th>\n",
       "      <td>Fake meat, free markets ease North Koreans' hunger</td>\n",
       "      <td>[(Fake, NNP), (meat, NN), (,, ,), (free, JJ), (markets, NNS), (ease, VBP), (North, NNP), (Koreans, NNP), (', POS), (hunger, NN)]</td>\n",
       "      <td>[fake, meat, free, markets, ease, north, koreans, hunger]</td>\n",
       "      <td>[fake, meat, free, market, ease, north, korean, hunger]</td>\n",
       "      <td>[NNP, NN, JJ, NNS, VBP, NNP, NNP, NN]</td>\n",
       "      <td>Fake meat, free markets ease North Koreans' hunger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Abadi defends role of Iranian-backed paramiltaries at meeting with Tillerson</td>\n",
       "      <td>[(Abadi, NNP), (defends, VBZ), (role, NN), (of, IN), (Iranian-backed, JJ), (paramiltaries, NNS), (at, IN), (meeting, VBG), (with, IN), (Tillerson, NNP)]</td>\n",
       "      <td>[abadi, defends, role, of, iranian backed, paramiltaries, at, meeting, with, tillerson]</td>\n",
       "      <td>[abadi, defend, role, iranian backed, paramiltaries, meet, tillerson]</td>\n",
       "      <td>[NNP, VBZ, NN, IN, JJ, NNS, IN, VBG, IN, NNP]</td>\n",
       "      <td>Abadi defends role of Iranian-backed paramiltaries at meeting with Tillerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>_u_s_ top court weighs race challenges to legislative districts</td>\n",
       "      <td>[(_u_s_, JJ), (top, JJ), (court, NN), (weighs, VBD), (race, NN), (challenges, NNS), (to, TO), (legislative, JJ), (districts, NNS)]</td>\n",
       "      <td>[_u_s_, top, court, weighs, race, challenges, to, legislative, districts]</td>\n",
       "      <td>[_u_s_, top, court, weigh, race, challenge, legislative, district]</td>\n",
       "      <td>[JJ, JJ, NN, VBD, NN, NNS, TO, JJ, NNS]</td>\n",
       "      <td>U.S. top court weighs race challenges to legislative districts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>Republicans retreat from plan to curb some press camera access in _u_s_ Capitol</td>\n",
       "      <td>[(Republicans, NNPS), (retreat, VBP), (from, IN), (plan, NN), (to, TO), (curb, VB), (some, DT), (press, NN), (camera, NN), (access, NN), (in, IN), (_u_s_, NNP), (Capitol, NNP)]</td>\n",
       "      <td>[republicans, retreat, from, plan, to, curb, some, press, camera, access, in, _u_s_, capitol]</td>\n",
       "      <td>[republican, retreat, plan, curb, press, camera, access, _u_s_, capitol]</td>\n",
       "      <td>[NNPS, VBP, IN, NN, TO, VB, DT, NN, NN, NN, IN, NNP, NNP]</td>\n",
       "      <td>Republicans retreat from plan to curb some press camera access in U.S. Capitol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>Trump refugee order dashes hopes of Iraqis who helped the _u_s_</td>\n",
       "      <td>[(Trump, NNP), (refugee, NN), (order, NN), (dashes, NNS), (hopes, NNS), (of, IN), (Iraqis, NNP), (who, WP), (helped, VBD), (the, DT), (_u_s_, NN)]</td>\n",
       "      <td>[trump, refugee, order, dashes, hopes, of, iraqis, who, helped, the, _u_s_]</td>\n",
       "      <td>[trump, refugee, order, dash, hope, iraqi, help, _u_s_]</td>\n",
       "      <td>[NNP, NN, NN, NNS, NNS, IN, NNP, WP, VBD, DT, NN]</td>\n",
       "      <td>Trump refugee order dashes hopes of Iraqis who helped the U.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Trump: 'No one really knows' how much tax plan would generate</td>\n",
       "      <td>[(Trump, NN), (:, :), ('No, CC), (one, CD), (really, RB), (knows, VBZ), (', POS), (how, WRB), (much, JJ), (tax, NN), (plan, NN), (would, MD), (generate, VB)]</td>\n",
       "      <td>[trump, :, no, one, really, knows, how, much, tax, plan, would, generate]</td>\n",
       "      <td>[trump, one, really, know, much, tax, plan, would, generate]</td>\n",
       "      <td>[NN, :, CC, CD, RB, VBZ, WRB, JJ, NN, NN, MD, VB]</td>\n",
       "      <td>Trump: 'No one really knows' how much tax plan would generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16230</th>\n",
       "      <td>Yemeni Salafist imam killed in Aden: sources</td>\n",
       "      <td>[(Yemeni, NNP), (Salafist, NNP), (imam, NN), (killed, VBN), (in, IN), (Aden, NNP), (:, :), (sources, NNS)]</td>\n",
       "      <td>[yemeni, salafist, imam, killed, in, aden, :, sources]</td>\n",
       "      <td>[yemeni, salafist, imam, kill, aden, source]</td>\n",
       "      <td>[NNP, NNP, NN, VBN, IN, NNP, :, NNS]</td>\n",
       "      <td>Yemeni Salafist imam killed in Aden: sources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    title  \\\n",
       "16151        Kurdish oil flows to Turkey resume after technical stoppage: shipping source   \n",
       "21001                       Venezuelan President Maduro will not go to _u_n_ rights forum   \n",
       "1679   Anti-Assad nations say no to Syria reconstruction until political process on track   \n",
       "15718                                  Fake meat, free markets ease North Koreans' hunger   \n",
       "1085         Abadi defends role of Iranian-backed paramiltaries at meeting with Tillerson   \n",
       "6906                      _u_s_ top court weighs race challenges to legislative districts   \n",
       "3223      Republicans retreat from plan to curb some press camera access in _u_s_ Capitol   \n",
       "5930                     Trump refugee order dashes hopes of Iraqis who helped the _u_s_    \n",
       "1075                        Trump: 'No one really knows' how much tax plan would generate   \n",
       "16230                                        Yemeni Salafist imam killed in Aden: sources   \n",
       "\n",
       "                                                                                                                                                                                    pos  \\\n",
       "16151               [(Kurdish, JJ), (oil, NN), (flows, VBZ), (to, TO), (Turkey, NNP), (resume, NN), (after, IN), (technical, JJ), (stoppage, NN), (:, :), (shipping, NN), (source, NN)]   \n",
       "21001                                          [(Venezuelan, NNP), (President, NNP), (Maduro, NNP), (will, MD), (not, RB), (go, VB), (to, TO), (_u_n_, VB), (rights, NNS), (forum, NN)]   \n",
       "1679         [(Anti-Assad, JJ), (nations, NNS), (say, VBP), (no, DT), (to, TO), (Syria, NNP), (reconstruction, NN), (until, IN), (political, JJ), (process, NN), (on, IN), (track, NN)]   \n",
       "15718                                                  [(Fake, NNP), (meat, NN), (,, ,), (free, JJ), (markets, NNS), (ease, VBP), (North, NNP), (Koreans, NNP), (', POS), (hunger, NN)]   \n",
       "1085                           [(Abadi, NNP), (defends, VBZ), (role, NN), (of, IN), (Iranian-backed, JJ), (paramiltaries, NNS), (at, IN), (meeting, VBG), (with, IN), (Tillerson, NNP)]   \n",
       "6906                                                 [(_u_s_, JJ), (top, JJ), (court, NN), (weighs, VBD), (race, NN), (challenges, NNS), (to, TO), (legislative, JJ), (districts, NNS)]   \n",
       "3223   [(Republicans, NNPS), (retreat, VBP), (from, IN), (plan, NN), (to, TO), (curb, VB), (some, DT), (press, NN), (camera, NN), (access, NN), (in, IN), (_u_s_, NNP), (Capitol, NNP)]   \n",
       "5930                                 [(Trump, NNP), (refugee, NN), (order, NN), (dashes, NNS), (hopes, NNS), (of, IN), (Iraqis, NNP), (who, WP), (helped, VBD), (the, DT), (_u_s_, NN)]   \n",
       "1075                      [(Trump, NN), (:, :), ('No, CC), (one, CD), (really, RB), (knows, VBZ), (', POS), (how, WRB), (much, JJ), (tax, NN), (plan, NN), (would, MD), (generate, VB)]   \n",
       "16230                                                                        [(Yemeni, NNP), (Salafist, NNP), (imam, NN), (killed, VBN), (in, IN), (Aden, NNP), (:, :), (sources, NNS)]   \n",
       "\n",
       "                                                                                         cleaned_words  \\\n",
       "16151       [kurdish, oil, flows, to, turkey, resume, after, technical, stoppage, :, shipping, source]   \n",
       "21001                         [venezuelan, president, maduro, will, not, go, to, _u_n_, rights, forum]   \n",
       "1679   [anti assad, nations, say, no, to, syria, reconstruction, until, political, process, on, track]   \n",
       "15718                                        [fake, meat, free, markets, ease, north, koreans, hunger]   \n",
       "1085           [abadi, defends, role, of, iranian backed, paramiltaries, at, meeting, with, tillerson]   \n",
       "6906                         [_u_s_, top, court, weighs, race, challenges, to, legislative, districts]   \n",
       "3223     [republicans, retreat, from, plan, to, curb, some, press, camera, access, in, _u_s_, capitol]   \n",
       "5930                       [trump, refugee, order, dashes, hopes, of, iraqis, who, helped, the, _u_s_]   \n",
       "1075                         [trump, :, no, one, really, knows, how, much, tax, plan, would, generate]   \n",
       "16230                                           [yemeni, salafist, imam, killed, in, aden, :, sources]   \n",
       "\n",
       "                                                                     minimal_words  \\\n",
       "16151  [kurdish, oil, flow, turkey, resume, technical, stoppage, shipping, source]   \n",
       "21001                         [venezuelan, president, maduro, _u_n_, right, forum]   \n",
       "1679   [anti assad, nation, say, syria, reconstruction, political, process, track]   \n",
       "15718                      [fake, meat, free, market, ease, north, korean, hunger]   \n",
       "1085         [abadi, defend, role, iranian backed, paramiltaries, meet, tillerson]   \n",
       "6906            [_u_s_, top, court, weigh, race, challenge, legislative, district]   \n",
       "3223      [republican, retreat, plan, curb, press, camera, access, _u_s_, capitol]   \n",
       "5930                       [trump, refugee, order, dash, hope, iraqi, help, _u_s_]   \n",
       "1075                  [trump, one, really, know, much, tax, plan, would, generate]   \n",
       "16230                                 [yemeni, salafist, imam, kill, aden, source]   \n",
       "\n",
       "                                                     cleaned_pos  \\\n",
       "16151          [JJ, NN, VBZ, TO, NNP, NN, IN, JJ, NN, :, NN, NN]   \n",
       "21001               [NNP, NNP, NNP, MD, RB, VB, TO, VB, NNS, NN]   \n",
       "1679         [JJ, NNS, VBP, DT, TO, NNP, NN, IN, JJ, NN, IN, NN]   \n",
       "15718                      [NNP, NN, JJ, NNS, VBP, NNP, NNP, NN]   \n",
       "1085               [NNP, VBZ, NN, IN, JJ, NNS, IN, VBG, IN, NNP]   \n",
       "6906                     [JJ, JJ, NN, VBD, NN, NNS, TO, JJ, NNS]   \n",
       "3223   [NNPS, VBP, IN, NN, TO, VB, DT, NN, NN, NN, IN, NNP, NNP]   \n",
       "5930           [NNP, NN, NN, NNS, NNS, IN, NNP, WP, VBD, DT, NN]   \n",
       "1075           [NN, :, CC, CD, RB, VBZ, WRB, JJ, NN, NN, MD, VB]   \n",
       "16230                       [NNP, NNP, NN, VBN, IN, NNP, :, NNS]   \n",
       "\n",
       "                                                                                org_title  \n",
       "16151        Kurdish oil flows to Turkey resume after technical stoppage: shipping source  \n",
       "21001                        Venezuelan President Maduro will not go to U.N. rights forum  \n",
       "1679   Anti-Assad nations say no to Syria reconstruction until political process on track  \n",
       "15718                                  Fake meat, free markets ease North Koreans' hunger  \n",
       "1085         Abadi defends role of Iranian-backed paramiltaries at meeting with Tillerson  \n",
       "6906                       U.S. top court weighs race challenges to legislative districts  \n",
       "3223       Republicans retreat from plan to curb some press camera access in U.S. Capitol  \n",
       "5930                       Trump refugee order dashes hopes of Iraqis who helped the U.S.  \n",
       "1075                        Trump: 'No one really knows' how much tax plan would generate  \n",
       "16230                                        Yemeni Salafist imam killed in Aden: sources  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pos</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>minimal_words</th>\n",
       "      <th>cleaned_pos</th>\n",
       "      <th>org_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>Trump Gets His _mytag_slans_ Handed To Him For Running Off Stage Because Some Guy Had A Sign</td>\n",
       "      <td>[(Trump, NNP), (Gets, VBZ), (His, PRP$), (_mytag_slans_, NN), (Handed, VBD), (To, TO), (Him, NNP), (For, IN), (Running, VBG), (Off, NNP), (Stage, NN), (Because, IN), (Some, DT), (Guy, NNP), (Had, ...</td>\n",
       "      <td>[trump, gets, his, _mytag_slans_, handed, to, him, for, running, off, stage, because, some, guy, had, a, sign]</td>\n",
       "      <td>[trump, get, _mytag_slans_, hand, run, stage, guy, sign]</td>\n",
       "      <td>[NNP, VBZ, PRP$, NN, VBD, TO, NNP, IN, VBG, NNP, NN, IN, DT, NNP, VBD, DT, NN]</td>\n",
       "      <td>Trump Gets His A** Handed To Him For Running Off Stage Because Some Guy Had A Sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20409</th>\n",
       "      <td>OBAMA COMMENCEMENT SPEECH To Black Graduates: You’re Just Lucky</td>\n",
       "      <td>[(OBAMA, NNP), (COMMENCEMENT, NNP), (SPEECH, NNP), (To, TO), (Black, NNP), (Graduates, NNP), (:, :), (You’re, NN), (Just, NNP), (Lucky, NNP)]</td>\n",
       "      <td>[obama, commencement, speech, to, black, graduates, :, you re, just, lucky]</td>\n",
       "      <td>[obama, commencement, speech, black, graduate, lucky]</td>\n",
       "      <td>[NNP, NNP, NNP, TO, NNP, NNP, :, NN, NNP, NNP]</td>\n",
       "      <td>OBAMA COMMENCEMENT SPEECH To Black Graduates: You’re Just Lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Trump Just Bragged About The Size Of His Hands At Hurricane Irma Relief Event  _mytag_parentheses_</td>\n",
       "      <td>[(Trump, NNP), (Just, NNP), (Bragged, NNP), (About, IN), (The, DT), (Size, NN), (Of, IN), (His, PRP$), (Hands, NNS), (At, IN), (Hurricane, NNP), (Irma, NNP), (Relief, NNP), (Event, NNP), (_mytag_p...</td>\n",
       "      <td>[trump, just, bragged, about, the, size, of, his, hands, at, hurricane, irma, relief, event, _mytag_parentheses_]</td>\n",
       "      <td>[trump, bragged, size, hand, hurricane, irma, relief, event, _mytag_parentheses_]</td>\n",
       "      <td>[NNP, NNP, NNP, IN, DT, NN, IN, PRP$, NNS, IN, NNP, NNP, NNP, NNP, NN]</td>\n",
       "      <td>Trump Just Bragged About The Size Of His Hands At Hurricane Irma Relief Event (VIDEO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13583</th>\n",
       "      <td>BEWARE OF HILLARY CLINTON’S “Smart Power” Foreign Policy…It’s NOT Smart!  _mytag_parentheses_</td>\n",
       "      <td>[(BEWARE, NNP), (OF, NNP), (HILLARY, NNP), (CLINTON’S, NNP), (“Smart, NNP), (Power”, NNP), (Foreign, NNP), (Policy…It’s, NNP), (NOT, NNP), (Smart, NNP), (!, .), (_mytag_parentheses_, NN)]</td>\n",
       "      <td>[beware, of, hillary, clinton s, smart, power, foreign, policy it s, not, smart, !, _mytag_parentheses_]</td>\n",
       "      <td>[beware, hillary, clinton, smart, power, foreign, policy, smart, _mytag_parentheses_]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, ., NN]</td>\n",
       "      <td>BEWARE OF HILLARY CLINTON’S “Smart Power” Foreign Policy…It’s NOT Smart! [VIDEO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>Trump Supporter Pulls Gun On Man Because He Refused To Vote For Trump</td>\n",
       "      <td>[(Trump, NNP), (Supporter, NNP), (Pulls, NNP), (Gun, NNP), (On, IN), (Man, NNP), (Because, IN), (He, PRP), (Refused, VBD), (To, TO), (Vote, VB), (For, IN), (Trump, NNP)]</td>\n",
       "      <td>[trump, supporter, pulls, gun, on, man, because, he, refused, to, vote, for, trump]</td>\n",
       "      <td>[trump, supporter, pull, gun, man, refuse, vote, trump]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, IN, NNP, IN, PRP, VBD, TO, VB, IN, NNP]</td>\n",
       "      <td>Trump Supporter Pulls Gun On Man Because He Refused To Vote For Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>Sean Spicer Dreams Up Brand-New Fake Terrorist Attack To Defend Trump’s Muslim Ban</td>\n",
       "      <td>[(Sean, JJ), (Spicer, NNP), (Dreams, NNP), (Up, NNP), (Brand-New, NNP), (Fake, NNP), (Terrorist, NNP), (Attack, NNP), (To, TO), (Defend, VB), (Trump’s, NNP), (Muslim, NNP), (Ban, NNP)]</td>\n",
       "      <td>[sean, spicer, dreams, up, brand new, fake, terrorist, attack, to, defend, trump s, muslim, ban]</td>\n",
       "      <td>[sean, spicer, dream, brand new, fake, terrorist, attack, defend, trump, muslim, ban]</td>\n",
       "      <td>[JJ, NNP, NNP, NNP, NNP, NNP, NNP, NNP, TO, VB, NNP, NNP, NNP]</td>\n",
       "      <td>Sean Spicer Dreams Up Brand-New Fake Terrorist Attack To Defend Trump’s Muslim Ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18009</th>\n",
       "      <td>N KOREA JUST REVEALED Plans To Unleash An Unimaginable  Attack That Could Lead To Electronic Armageddon</td>\n",
       "      <td>[(N, NNP), (KOREA, NNP), (JUST, NNP), (REVEALED, NNP), (Plans, NNPS), (To, TO), (Unleash, VB), (An, DT), (Unimaginable, JJ), (Attack, NN), (That, WDT), (Could, NNP), (Lead, NNP), (To, TO), (Electr...</td>\n",
       "      <td>[n, korea, just, revealed, plans, to, unleash, an, unimaginable, attack, that, could, lead, to, electronic, armageddon]</td>\n",
       "      <td>[korea, revealed, plan, unleash, unimaginable, attack, could, lead, electronic, armageddon]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNPS, TO, VB, DT, JJ, NN, WDT, NNP, NNP, TO, NNP, NNP]</td>\n",
       "      <td>N KOREA JUST REVEALED Plans To Unleash An Unimaginable  Attack That Could Lead To Electronic Armageddon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22023</th>\n",
       "      <td>Zakharova Slams CIA Chief Pompeo: Stop Making Up Anti-Russian Fiction</td>\n",
       "      <td>[(Zakharova, NNP), (Slams, NNP), (CIA, NNP), (Chief, NNP), (Pompeo, NNP), (:, :), (Stop, NN), (Making, VBG), (Up, RP), (Anti-Russian, JJ), (Fiction, NN)]</td>\n",
       "      <td>[zakharova, slams, cia, chief, pompeo, :, stop, making, up, anti russian, fiction]</td>\n",
       "      <td>[zakharova, slam, cia, chief, pompeo, stop, make, anti russian, fiction]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, :, NN, VBG, RP, JJ, NN]</td>\n",
       "      <td>Zakharova Slams CIA Chief Pompeo: Stop Making Up Anti-Russian Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>Democrat Corey Booker Backs Single-Payer and Wide Open Borders: ‘Build Tunnels, Not Walls’  _mytag_parentheses_</td>\n",
       "      <td>[(Democrat, NNP), (Corey, NNP), (Booker, NNP), (Backs, NNP), (Single-Payer, NNP), (and, CC), (Wide, NNP), (Open, NNP), (Borders, NNS), (:, :), (‘Build, NN), (Tunnels, NNP), (,, ,), (Not, RB), (Wal...</td>\n",
       "      <td>[democrat, corey, booker, backs, single payer, and, wide, open, borders, :, build, tunnels, not, walls, _mytag_parentheses_]</td>\n",
       "      <td>[democrat, corey, booker, back, single payer, wide, open, border, build, tunnel, wall, _mytag_parentheses_]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, CC, NNP, NNP, NNS, :, NN, NNP, RB, NNP, NN]</td>\n",
       "      <td>Democrat Corey Booker Backs Single-Payer and Wide Open Borders: ‘Build Tunnels, Not Walls’ [Video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15729</th>\n",
       "      <td>HAS FACE BOOK SIDED WITH MUSLIM JIHADISTS AGAINST FREE SPEECH? Muhammed Cartoon Contest Winner Is Removed From Social Media Site</td>\n",
       "      <td>[(HAS, NNP), (FACE, NNP), (BOOK, NNP), (SIDED, NNP), (WITH, NNP), (MUSLIM, NNP), (JIHADISTS, NNP), (AGAINST, NNP), (FREE, NNP), (SPEECH, NNP), (?, .), (Muhammed, NNP), (Cartoon, NNP), (Contest, NN...</td>\n",
       "      <td>[has, face, book, sided, with, muslim, jihadists, against, free, speech, ?, muhammed, cartoon, contest, winner, is, removed, from, social, media, site]</td>\n",
       "      <td>[face, book, sided, muslim, jihadist, free, speech, muhammed, cartoon, contest, winner, remove, social, medium, site]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, ., NNP, NNP, NNP, NNP, VBZ, VBN, IN, NNP, NNP, NNP]</td>\n",
       "      <td>HAS FACE BOOK SIDED WITH MUSLIM JIHADISTS AGAINST FREE SPEECH? Muhammed Cartoon Contest Winner Is Removed From Social Media Site</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  title  \\\n",
       "3915                                       Trump Gets His _mytag_slans_ Handed To Him For Running Off Stage Because Some Guy Had A Sign   \n",
       "20409                                                                   OBAMA COMMENCEMENT SPEECH To Black Graduates: You’re Just Lucky   \n",
       "379                                 Trump Just Bragged About The Size Of His Hands At Hurricane Irma Relief Event  _mytag_parentheses_    \n",
       "13583                                    BEWARE OF HILLARY CLINTON’S “Smart Power” Foreign Policy…It’s NOT Smart!  _mytag_parentheses_    \n",
       "3883                                                              Trump Supporter Pulls Gun On Man Because He Refused To Vote For Trump   \n",
       "2617                                                 Sean Spicer Dreams Up Brand-New Fake Terrorist Attack To Defend Trump’s Muslim Ban   \n",
       "18009                           N KOREA JUST REVEALED Plans To Unleash An Unimaginable  Attack That Could Lead To Electronic Armageddon   \n",
       "22023                                                             Zakharova Slams CIA Chief Pompeo: Stop Making Up Anti-Russian Fiction   \n",
       "15946                  Democrat Corey Booker Backs Single-Payer and Wide Open Borders: ‘Build Tunnels, Not Walls’  _mytag_parentheses_    \n",
       "15729  HAS FACE BOOK SIDED WITH MUSLIM JIHADISTS AGAINST FREE SPEECH? Muhammed Cartoon Contest Winner Is Removed From Social Media Site   \n",
       "\n",
       "                                                                                                                                                                                                           pos  \\\n",
       "3915   [(Trump, NNP), (Gets, VBZ), (His, PRP$), (_mytag_slans_, NN), (Handed, VBD), (To, TO), (Him, NNP), (For, IN), (Running, VBG), (Off, NNP), (Stage, NN), (Because, IN), (Some, DT), (Guy, NNP), (Had, ...   \n",
       "20409                                                            [(OBAMA, NNP), (COMMENCEMENT, NNP), (SPEECH, NNP), (To, TO), (Black, NNP), (Graduates, NNP), (:, :), (You’re, NN), (Just, NNP), (Lucky, NNP)]   \n",
       "379    [(Trump, NNP), (Just, NNP), (Bragged, NNP), (About, IN), (The, DT), (Size, NN), (Of, IN), (His, PRP$), (Hands, NNS), (At, IN), (Hurricane, NNP), (Irma, NNP), (Relief, NNP), (Event, NNP), (_mytag_p...   \n",
       "13583              [(BEWARE, NNP), (OF, NNP), (HILLARY, NNP), (CLINTON’S, NNP), (“Smart, NNP), (Power”, NNP), (Foreign, NNP), (Policy…It’s, NNP), (NOT, NNP), (Smart, NNP), (!, .), (_mytag_parentheses_, NN)]   \n",
       "3883                                 [(Trump, NNP), (Supporter, NNP), (Pulls, NNP), (Gun, NNP), (On, IN), (Man, NNP), (Because, IN), (He, PRP), (Refused, VBD), (To, TO), (Vote, VB), (For, IN), (Trump, NNP)]   \n",
       "2617                  [(Sean, JJ), (Spicer, NNP), (Dreams, NNP), (Up, NNP), (Brand-New, NNP), (Fake, NNP), (Terrorist, NNP), (Attack, NNP), (To, TO), (Defend, VB), (Trump’s, NNP), (Muslim, NNP), (Ban, NNP)]   \n",
       "18009  [(N, NNP), (KOREA, NNP), (JUST, NNP), (REVEALED, NNP), (Plans, NNPS), (To, TO), (Unleash, VB), (An, DT), (Unimaginable, JJ), (Attack, NN), (That, WDT), (Could, NNP), (Lead, NNP), (To, TO), (Electr...   \n",
       "22023                                                [(Zakharova, NNP), (Slams, NNP), (CIA, NNP), (Chief, NNP), (Pompeo, NNP), (:, :), (Stop, NN), (Making, VBG), (Up, RP), (Anti-Russian, JJ), (Fiction, NN)]   \n",
       "15946  [(Democrat, NNP), (Corey, NNP), (Booker, NNP), (Backs, NNP), (Single-Payer, NNP), (and, CC), (Wide, NNP), (Open, NNP), (Borders, NNS), (:, :), (‘Build, NN), (Tunnels, NNP), (,, ,), (Not, RB), (Wal...   \n",
       "15729  [(HAS, NNP), (FACE, NNP), (BOOK, NNP), (SIDED, NNP), (WITH, NNP), (MUSLIM, NNP), (JIHADISTS, NNP), (AGAINST, NNP), (FREE, NNP), (SPEECH, NNP), (?, .), (Muhammed, NNP), (Cartoon, NNP), (Contest, NN...   \n",
       "\n",
       "                                                                                                                                                 cleaned_words  \\\n",
       "3915                                            [trump, gets, his, _mytag_slans_, handed, to, him, for, running, off, stage, because, some, guy, had, a, sign]   \n",
       "20409                                                                              [obama, commencement, speech, to, black, graduates, :, you re, just, lucky]   \n",
       "379                                          [trump, just, bragged, about, the, size, of, his, hands, at, hurricane, irma, relief, event, _mytag_parentheses_]   \n",
       "13583                                                 [beware, of, hillary, clinton s, smart, power, foreign, policy it s, not, smart, !, _mytag_parentheses_]   \n",
       "3883                                                                       [trump, supporter, pulls, gun, on, man, because, he, refused, to, vote, for, trump]   \n",
       "2617                                                          [sean, spicer, dreams, up, brand new, fake, terrorist, attack, to, defend, trump s, muslim, ban]   \n",
       "18009                                  [n, korea, just, revealed, plans, to, unleash, an, unimaginable, attack, that, could, lead, to, electronic, armageddon]   \n",
       "22023                                                                       [zakharova, slams, cia, chief, pompeo, :, stop, making, up, anti russian, fiction]   \n",
       "15946                             [democrat, corey, booker, backs, single payer, and, wide, open, borders, :, build, tunnels, not, walls, _mytag_parentheses_]   \n",
       "15729  [has, face, book, sided, with, muslim, jihadists, against, free, speech, ?, muhammed, cartoon, contest, winner, is, removed, from, social, media, site]   \n",
       "\n",
       "                                                                                                               minimal_words  \\\n",
       "3915                                                                [trump, get, _mytag_slans_, hand, run, stage, guy, sign]   \n",
       "20409                                                                  [obama, commencement, speech, black, graduate, lucky]   \n",
       "379                                        [trump, bragged, size, hand, hurricane, irma, relief, event, _mytag_parentheses_]   \n",
       "13583                                  [beware, hillary, clinton, smart, power, foreign, policy, smart, _mytag_parentheses_]   \n",
       "3883                                                                 [trump, supporter, pull, gun, man, refuse, vote, trump]   \n",
       "2617                                   [sean, spicer, dream, brand new, fake, terrorist, attack, defend, trump, muslim, ban]   \n",
       "18009                            [korea, revealed, plan, unleash, unimaginable, attack, could, lead, electronic, armageddon]   \n",
       "22023                                               [zakharova, slam, cia, chief, pompeo, stop, make, anti russian, fiction]   \n",
       "15946            [democrat, corey, booker, back, single payer, wide, open, border, build, tunnel, wall, _mytag_parentheses_]   \n",
       "15729  [face, book, sided, muslim, jihadist, free, speech, muhammed, cartoon, contest, winner, remove, social, medium, site]   \n",
       "\n",
       "                                                                                                  cleaned_pos  \\\n",
       "3915                           [NNP, VBZ, PRP$, NN, VBD, TO, NNP, IN, VBG, NNP, NN, IN, DT, NNP, VBD, DT, NN]   \n",
       "20409                                                          [NNP, NNP, NNP, TO, NNP, NNP, :, NN, NNP, NNP]   \n",
       "379                                    [NNP, NNP, NNP, IN, DT, NN, IN, PRP$, NNS, IN, NNP, NNP, NNP, NNP, NN]   \n",
       "13583                                               [NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, ., NN]   \n",
       "3883                                             [NNP, NNP, NNP, NNP, IN, NNP, IN, PRP, VBD, TO, VB, IN, NNP]   \n",
       "2617                                           [JJ, NNP, NNP, NNP, NNP, NNP, NNP, NNP, TO, VB, NNP, NNP, NNP]   \n",
       "18009                             [NNP, NNP, NNP, NNP, NNPS, TO, VB, DT, JJ, NN, WDT, NNP, NNP, TO, NNP, NNP]   \n",
       "22023                                                       [NNP, NNP, NNP, NNP, NNP, :, NN, VBG, RP, JJ, NN]   \n",
       "15946                                   [NNP, NNP, NNP, NNP, NNP, CC, NNP, NNP, NNS, :, NN, NNP, RB, NNP, NN]   \n",
       "15729  [NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, NNP, ., NNP, NNP, NNP, NNP, VBZ, VBN, IN, NNP, NNP, NNP]   \n",
       "\n",
       "                                                                                                                              org_title  \n",
       "3915                                                 Trump Gets His A** Handed To Him For Running Off Stage Because Some Guy Had A Sign  \n",
       "20409                                                                   OBAMA COMMENCEMENT SPEECH To Black Graduates: You’re Just Lucky  \n",
       "379                                               Trump Just Bragged About The Size Of His Hands At Hurricane Irma Relief Event (VIDEO)  \n",
       "13583                                                  BEWARE OF HILLARY CLINTON’S “Smart Power” Foreign Policy…It’s NOT Smart! [VIDEO]  \n",
       "3883                                                              Trump Supporter Pulls Gun On Man Because He Refused To Vote For Trump  \n",
       "2617                                                 Sean Spicer Dreams Up Brand-New Fake Terrorist Attack To Defend Trump’s Muslim Ban  \n",
       "18009                           N KOREA JUST REVEALED Plans To Unleash An Unimaginable  Attack That Could Lead To Electronic Armageddon  \n",
       "22023                                                             Zakharova Slams CIA Chief Pompeo: Stop Making Up Anti-Russian Fiction  \n",
       "15946                                Democrat Corey Booker Backs Single-Payer and Wide Open Borders: ‘Build Tunnels, Not Walls’ [Video]  \n",
       "15729  HAS FACE BOOK SIDED WITH MUSLIM JIHADISTS AGAINST FREE SPEECH? Muhammed Cartoon Contest Winner Is Removed From Social Media Site  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df0.sample(10))\n",
    "display(df1.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cb462702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_csv('TrueOrganized')\n",
    "df1.to_csv('FakeOrganized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4599ef",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We finished cleaning and organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7963e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
