{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cfc5f3",
   "metadata": {},
   "source": [
    "# Training hyperparameter tuning\n",
    "Due to limited computing resources, let's be satisfied for course tuning.\n",
    "### List of hyperparameters\n",
    "- Model architecture\n",
    "    - n_units: number of hidden layers and units\n",
    "- Training\n",
    "    - learning_rate\n",
    "    - batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115458bc",
   "metadata": {},
   "source": [
    "# Load libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ecafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freq_utils import *\n",
    "\n",
    "import regex as re\n",
    "import IPython\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0880d1",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb39b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('data/TrueOrganized.csv')\n",
    "df1 = pd.read_csv('data/FakeOrganized.csv')\n",
    "df0['label'] = 0\n",
    "df1['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203bc01",
   "metadata": {},
   "source": [
    "# Make word encoding items\n",
    "- Make dictionaries\n",
    "- Get max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ee40a",
   "metadata": {},
   "source": [
    "### Pretrained word embeddings\n",
    "- Word to index\n",
    "- Word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d68c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vector = get_pretrained_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a4814",
   "metadata": {},
   "source": [
    "### PoS tag encodings\n",
    "- PoS word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c9f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 {'dt': 0, 'nns': 1, 'vbz': 2, '(': 3, 'vb': 4, 'vbg': 5, 'sym': 6, 'cd': 7, 'jjr': 8, ')': 9, 'wp': 10, 'rbs': 11, 'rb': 12, ',': 13, 'in': 14, ':': 15, 'rbr': 16, '.': 17, 'vbn': 18, 'ex': 19, 'fw': 20, 'vbd': 21, 'jj': 22, 'prp$': 23, 'md': 24, 'pos': 25, 'wdt': 26, 'jjs': 27, 'prp': 28, 'vbp': 29, \"''\": 30, 'nn': 31, 'uh': 32, 'nnp': 33, 'to': 34, 'cc': 35, 'wp$': 36, 'nnps': 37, 'wrb': 38}\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df0.cleaned_pos, df1.cleaned_pos])\n",
    "\n",
    "pos_set = set()\n",
    "for x in list(df.str.lower().str.split()):\n",
    "    pos_set.update(x)\n",
    "\n",
    "pos_list = list(pos_set)\n",
    "pos_to_index = { pos_list[i]: i for i in range(len(pos_list)) }\n",
    "\n",
    "print(len(pos_to_index),pos_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861645ac",
   "metadata": {},
   "source": [
    "### Get max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57daf8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_title \t 20 42\n",
      "lower_title \t 20 42\n",
      "cleaned_words \t 24 49\n",
      "cleaned_pos \t 24 49\n",
      "minimal_words \t 15 35\n",
      "{'org_title': 42, 'lower_title': 42, 'cleaned_words': 49, 'cleaned_pos': 49, 'minimal_words': 35}\n"
     ]
    }
   ],
   "source": [
    "xcol_names = df0.columns[:-1].to_list()\n",
    "input_dict = {}\n",
    "for x in xcol_names:\n",
    "    print(x,'\\t', df0[x].str.split().str.len().max(), df1[x].str.split().str.len().max())\n",
    "    input_dict[x]=df1[x].str.split().str.len().max()\n",
    "print(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207d880",
   "metadata": {},
   "source": [
    "# Model layer segment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e930d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_encoder(X, trainable = True):\n",
    "    \n",
    "    # X input can be either text or PoS vectors\n",
    "    # dim=2 for text -> word index input (m, max_len)\n",
    "    # dim=3 for PoS -> one-hot encoding input (m, max_len, num_cat)\n",
    "    dim = len(X.get_shape().as_list())\n",
    "    \n",
    "    if dim==2:\n",
    "        # Word embedding, indices to vector\n",
    "        # Output: (m, max_len, emb_dim)\n",
    "        X = pretrained_embedding_layer(word_to_vector, word_to_index, trainable=trainable)(X)             \n",
    "    elif dim==3:\n",
    "        # Int to float for One-hot encoding\n",
    "        # Output: (m, max_len, num_cat)\n",
    "        X = tf.dtypes.cast(X, tf.float32)\n",
    "    else:\n",
    "        print('Wrong input shape:', X.get_shape())\n",
    "        \n",
    "    # Skip zero vector words\n",
    "    X = tfl.Masking(mask_value=0.)(X)    \n",
    "    \n",
    "    return X\n",
    "\n",
    "def ml_builder(X, hp, ml_type):\n",
    "\n",
    "    drop_out=0.3\n",
    "\n",
    "    if ml_type=='FNN':\n",
    "        \n",
    "        # Take average of a sentence\n",
    "        max_len = X.shape[1]\n",
    "        X_avg = [ X[:,i,:] for i in range(max_len) ]\n",
    "        X = tf.keras.layers.Average()(X_avg)    \n",
    "        \n",
    "        # Linear+ReLu layer\n",
    "        X = tfl.Dense(units = hp.Int('n_unit1', min_value = 128, max_value = 256, step = 128), \n",
    "                      activation='relu', kernel_initializer='he_normal')(X)\n",
    "        X = tfl.Dropout(rate = drop_out)(X)  \n",
    "        X = tfl.BatchNormalization()(X)\n",
    "        \n",
    "        X = tfl.Dense(units = hp.Int('n_unit2', min_value = 32, max_value = 64, step = 32), \n",
    "                      activation='relu', kernel_initializer='he_normal')(X)\n",
    "        X = tfl.Dropout(rate = drop_out)(X)  \n",
    "        X = tfl.BatchNormalization()(X)  \n",
    "        \n",
    "        X = tfl.Dense(units = 16, \n",
    "                      activation='relu', kernel_initializer='he_normal')(X)\n",
    "        X = tfl.Dropout(rate = drop_out)(X)  \n",
    "        X = tfl.BatchNormalization()(X)        \n",
    "        \n",
    "    elif ml_type=='LSTM':\n",
    "        \n",
    "        n_unit = hp.Choice('n_unit', values=[128,256])\n",
    "        \n",
    "        # Output: a[l] (m, max_len, # hidden unit), batch of sequences\n",
    "        X = tfl.LSTM(units = n_unit, dropout = drop_out, recurrent_dropout=drop_out, return_sequences= True)(X)\n",
    "        X = tfl.LSTM(units = n_unit, dropout = drop_out, recurrent_dropout=drop_out, return_sequences= True)(X)\n",
    "\n",
    "        # Output: a[l] (m, # hidden unit)\n",
    "        X = tfl.LSTM(units = n_unit, dropout = drop_out, recurrent_dropout=drop_out, return_sequences= False)(X)\n",
    "\n",
    "    else:\n",
    "        print('Wrong ml_type:',ml_type)\n",
    "        \n",
    "            \n",
    "    # Linear+Softmax layer\n",
    "    # Output: y (m, # classes=2), probability of each class\n",
    "    X = tfl.Dense(units = 2, activation='softmax')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def ml_optimizer(model, hp):\n",
    "    \n",
    "    # Hyperparameter\n",
    "    learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)                                \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43af39",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fd14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def __init__(self, input_shape, ml_type='LSTM', trainable=True):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.ml_type = ml_type\n",
    "        self.trainable = trainable\n",
    "    \n",
    "    def build(self, hp):\n",
    "\n",
    "        # X_oh (m, max_len, num_cat)\n",
    "        # X_indices (m, max_len)\n",
    "        X_input = tfl.Input(shape=self.input_shape, dtype='int32')\n",
    "\n",
    "        X = input_encoder(X_input, trainable = self.trainable)\n",
    "        X = ml_builder(X, hp, ml_type=self.ml_type)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=X_input, outputs=X)\n",
    "        model = ml_optimizer(model, hp)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [32, 64, 128]),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3053fed",
   "metadata": {},
   "source": [
    "# Generate inputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee4ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = train_dev_test_split([df0, df1], m=1000, class_column='label', \n",
    "                                    class_balance=True, r_dev=0.2, r_test=0.2, rand_state=42)\n",
    "\n",
    "def make_input(xcol_name):\n",
    "\n",
    "    max_len = input_dict[xcol_name]\n",
    "    use_embeddings = True\n",
    "    if xcol_name == 'cleaned_pos':\n",
    "        use_embeddings=False\n",
    "\n",
    "    w2i = False\n",
    "    w2v = False\n",
    "    X_shape = False\n",
    "\n",
    "    # Embedding or One-hot encoding\n",
    "    if use_embeddings:\n",
    "        w2i = word_to_index        \n",
    "        w2v = word_to_vector\n",
    "        X_shape = (max_len, )\n",
    "    else:\n",
    "        w2i = pos_to_index\n",
    "        X_shape = (max_len, len(pos_to_index))\n",
    "\n",
    "    _, _, X_train_indices, _, Y_train_oh = dataframe_to_arrays(train, w2i, max_len, Xname=xcol_name)\n",
    "    _, _, X_dev_indices,   _, Y_dev_oh   = dataframe_to_arrays(dev, w2i, max_len, Xname=xcol_name)\n",
    "    index, _, X_test_indices, Y_test, _  = dataframe_to_arrays(test, w2i, max_len, Xname=xcol_name)\n",
    "\n",
    "    # X, Y (train, dev, test)\n",
    "    X = False\n",
    "    if use_embeddings:\n",
    "        X = [X_train_indices, X_dev_indices, X_test_indices]\n",
    "    else:\n",
    "        X = [to_categorical(X_train_indices, num_classes=len(pos_to_index)), \n",
    "             to_categorical(X_dev_indices, num_classes=len(pos_to_index)), \n",
    "             to_categorical(X_test_indices, num_classes=len(pos_to_index))]            \n",
    "\n",
    "    Y = [Y_train_oh, Y_dev_oh, Y_test]\n",
    "    \n",
    "    print('input column:',xcol_name)\n",
    "    print('max_len:',max_len)\n",
    "    print('use_embeddings?:',use_embeddings)\n",
    "    print(np.shape(X[0]), X_shape)\n",
    "    print(np.shape(Y[0]))\n",
    "    \n",
    "    return X, Y, X_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f7910",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b68bb66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 Complete [00h 00m 10s]\n",
      "accuracy: 0.6033333539962769\n",
      "\n",
      "Best accuracy So Far: 0.878333330154419\n",
      "Total elapsed time: 00h 06m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)\n",
    "\n",
    "durations=[]\n",
    "        \n",
    "for ml_type in ('LSTM','FNN'):\n",
    "    for xcol_name in xcol_names:\n",
    "        \n",
    "        #if not (ml_type=='FNN' and xcol_name=='cleaned_pos'):\n",
    "        #    continue\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        \n",
    "        X, Y, X_shape = make_input(xcol_name)\n",
    "        \n",
    "        dir_name = ml_type+'_'+xcol_name\n",
    "        \n",
    "        tuner = kt.Hyperband(MyHyperModel(X_shape, ml_type=ml_type, trainable=True),\n",
    "                             objective = 'accuracy', \n",
    "                             max_epochs = 100,\n",
    "                             factor = 3,\n",
    "                             #directory = dir_name,\n",
    "                             overwrite = True)\n",
    "\n",
    "        tuner.search(X[0], Y[0],\n",
    "                     validation_data=(X[1], Y[1]),\n",
    "                     epochs=100,\n",
    "                     callbacks=[ClearTrainingOutput(), tf.keras.callbacks.EarlyStopping(patience=5)])\n",
    "\n",
    "        \n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "        \n",
    "        # commented out for too much disk space\n",
    "        #model = tuner.hypermodel.build(best_hps)\n",
    "        #save_name = dir_name+'/best_model'\n",
    "        #model.save(save_name)\n",
    "\n",
    "        save_name = 'data/'+dir_name+'_best_hps'\n",
    "        dump(best_hps,save_name)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        durations.append([ml_type,xcol_name,(end_time-begin_time)/60,'min.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db546c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['LSTM', 'org_title', 10.859543867905934, 'min.'], ['LSTM', 'lower_title', 9.799444234371185, 'min.'], ['LSTM', 'cleaned_words', 11.244206198056538, 'min.'], ['LSTM', 'cleaned_pos', 7.861250285307566, 'min.'], ['LSTM', 'minimal_words', 9.601969150702159, 'min.'], ['FNN', 'org_title', 6.706359946727753, 'min.'], ['FNN', 'lower_title', 6.971039915084839, 'min.'], ['FNN', 'cleaned_words', 7.571445182959239, 'min.'], ['FNN', 'cleaned_pos', 1.4807878176371256, 'min.'], ['FNN', 'minimal_words', 6.9601916670799255, 'min.']]\n"
     ]
    }
   ],
   "source": [
    "print(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99035679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
