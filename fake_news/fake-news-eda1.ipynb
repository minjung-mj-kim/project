{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "The goal of this EDA is to find extractable and useful features which can utilize the known characterastics of fake news, as well as to find unexpected features of fake news. There are tons of items to explore. Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.help import upenn_tagset\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def pline(word):\n",
    "    print('\\n===== ',word,' =====\\n')\n",
    "    \n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('True.csv')\n",
    "df1 = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data quality and contents\n",
    "Provided data are already well organized, so let's just see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian Collusion Investigation</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia inve...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’</td>\n",
       "      <td>On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for Homeland Security Secretary in Donald Trump s administration, has an email scandal of his own.In...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name Coded Into His Website (IMAGES)</td>\n",
       "      <td>On Christmas day, Donald Trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. The former reality show star blasted former President Bar...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump During His Christmas Speech</td>\n",
       "      <td>Pope Francis used his annual Christmas Day message to rebuke Donald Trump without even mentioning his name. The Pope delivered his message just days after members of the United Nations condemned T...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated US Sailors Well</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earlier this week, the unlikely  mishap  of two US Naval vessels straying into Iranian waters   just hours before the President s State of the Union speec...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-action: $4M for Lawyers, $0 for Users</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. Whenever there is a dispute or a change of law, and two tribes go to war, there is normally only one real winner after the tribulation  the lawyers. A...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to Take Territorial Booty in Northern Syria</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember when the Obama Administration told the world how it hoped to identify 5,000 reliable non-jihadist  moderate  rebels hanging out in Turkey and Jordan, ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America Finally Calls it Quits</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will go down in history as one of the biggest failures in broadcast media history.Ever since the US and its allies began plotting to overthrow Libya and S...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military – Signs of a Neocon Political Stunt</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in its new year s look ahead, we have a new  hostage  crisis underway.Today, Iranian military forces report that two small riverine U.S. Navy boats were ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            title  \\\n",
       "0                  Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing   \n",
       "1                            Drunk Bragging Trump Staffer Started Russian Collusion Investigation   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’   \n",
       "3                   Trump Is So Obsessed He Even Has Obama’s Name Coded Into His Website (IMAGES)   \n",
       "4                           Pope Francis Just Called Out Donald Trump During His Christmas Speech   \n",
       "...                                                                                           ...   \n",
       "23476                               McPain: John McCain Furious That Iran Treated US Sailors Well   \n",
       "23477           JUSTICE? Yahoo Settles E-mail Privacy Class-action: $4M for Lawyers, $0 for Users   \n",
       "23478       Sunnistan: US and Allied ‘Safe Zone’ Plan to Take Territorial Booty in Northern Syria   \n",
       "23479                         How to Blow $700 Million: Al Jazeera America Finally Calls it Quits   \n",
       "23480           10 U.S. Navy Sailors Held by Iranian Military – Signs of a Neocon Political Stunt   \n",
       "\n",
       "                                                                                                                                                                                                          text  \\\n",
       "0      Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former...   \n",
       "1      House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia inve...   \n",
       "2      On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for Homeland Security Secretary in Donald Trump s administration, has an email scandal of his own.In...   \n",
       "3      On Christmas day, Donald Trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. The former reality show star blasted former President Bar...   \n",
       "4      Pope Francis used his annual Christmas Day message to rebuke Donald Trump without even mentioning his name. The Pope delivered his message just days after members of the United Nations condemned T...   \n",
       "...                                                                                                                                                                                                        ...   \n",
       "23476  21st Century Wire says As 21WIRE reported earlier this week, the unlikely  mishap  of two US Naval vessels straying into Iranian waters   just hours before the President s State of the Union speec...   \n",
       "23477  21st Century Wire says It s a familiar theme. Whenever there is a dispute or a change of law, and two tribes go to war, there is normally only one real winner after the tribulation  the lawyers. A...   \n",
       "23478  Patrick Henningsen  21st Century WireRemember when the Obama Administration told the world how it hoped to identify 5,000 reliable non-jihadist  moderate  rebels hanging out in Turkey and Jordan, ...   \n",
       "23479  21st Century Wire says Al Jazeera America will go down in history as one of the biggest failures in broadcast media history.Ever since the US and its allies began plotting to overthrow Libya and S...   \n",
       "23480  21st Century Wire says As 21WIRE predicted in its new year s look ahead, we have a new  hostage  crisis underway.Today, Iranian military forces report that two small riverine U.S. Navy boats were ...   \n",
       "\n",
       "           subject               date  \n",
       "0             News  December 31, 2017  \n",
       "1             News  December 31, 2017  \n",
       "2             News  December 30, 2017  \n",
       "3             News  December 29, 2017  \n",
       "4             News  December 25, 2017  \n",
       "...            ...                ...  \n",
       "23476  Middle-east   January 16, 2016  \n",
       "23477  Middle-east   January 16, 2016  \n",
       "23478  Middle-east   January 15, 2016  \n",
       "23479  Middle-east   January 14, 2016  \n",
       "23480  Middle-east   January 12, 2016  \n",
       "\n",
       "[23478 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.drop_duplicates()\n",
    "df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print('\\n===== Format =====\\n')\n",
    "print(df0.info())\n",
    "print(df1.info())\n",
    "print('\\n  ---- Real, subject\\n',df0.subject.value_counts())\n",
    "print('\\n  ---- Fake, subject\\n',df1.subject.value_counts())\n",
    "print('\\n  ---- Real, date\\n',df0.date.nunique())\n",
    "print('\\n  ---- Fake, date\\n',df1.date.nunique())\n",
    "print('\\n  ---- Real, head\\n',df0.head(5))\n",
    "print('\\n  ---- Fake, head\\n',df1.head(5))\n",
    "\n",
    "print('\\n===== (Nearly) Empty contents =====\\n')\n",
    "print('\\n  ---- Real, text <6 words\\n')\n",
    "print(df0[df0.text.str.split().str.len()<6].text.count())\n",
    "try:\n",
    "    print(df0[df0.text.str.split().str.len()<6].sample(20))\n",
    "except:\n",
    "    print(df0[df0.text.str.split().str.len()<6].head(20))\n",
    "\n",
    "print('\\n  ---- Fake, text <6 words\\n')\n",
    "print(df1[df1.text.str.split().str.len()<6].text.count())\n",
    "try:\n",
    "    print(df1[df1.text.str.split().str.len()<6].sample(20))\n",
    "except:\n",
    "    print(df1[df1.text.str.split().str.len()<6].head(20))\n",
    "    \n",
    "print('\\n  ---- Real, title <3 words\\n')\n",
    "print(df0[df0.title.str.split().str.len()<3].title.count())\n",
    "try:\n",
    "    print(df0[df0.title.str.split().str.len()<3].sample(20))\n",
    "except:\n",
    "    print(df0[df0.title.str.split().str.len()<3].head(20))\n",
    "    \n",
    "print('\\n  ---- Fake, title <3 words\\n')\n",
    "print(df1[df1.title.str.split().str.len()<3].title.count())\n",
    "try:\n",
    "    print(df1[df1.title.str.split().str.len()<3].sample(20))\n",
    "except:\n",
    "    print(df1[df1.title.str.split().str.len()<3].head(20))\n",
    "\n",
    "print('\\n===== Title, Real =====\\n')\n",
    "print(df0.title.sample(30))\n",
    "print('\\n===== Title, Fake =====\\n')\n",
    "print(df1.title.sample(30))\n",
    "print('\\n===== Text, Real =====\\n')\n",
    "print(df0.text.sample(1))\n",
    "print('\\n===== Text, Fake =====\\n')\n",
    "print(df1.text.sample(1))\n",
    "\n",
    "\n",
    "print('\\n===== Fake news samples of each subject =====\\n')\n",
    "\n",
    "for x in df1.subject.unique():\n",
    "    print('\\n  ----',x,' \\n')\n",
    "    print(df1[df1.subject==x].drop(['subject','date'], axis=1, inplace=False).sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "1. Data format\n",
    "- 21k of real and 23k of fake news.\n",
    "- No duplication, no nan entry.\n",
    "\n",
    "2. Title\n",
    "- Every **real news** has a title. These titles are **concise and informative summaries** of the main artcle, so title is enough to guess the topic of contents.\n",
    "- **Fake news** have **longer** title, but often they are **teasers**, i.e. you **can't guess what happened without cliking the title** and see the contents.\n",
    "- **Empty title** of **fake news** are mostly a **website address**. I don't know if it's a **mistake of data processing or not**. I'll **discard** those rows because it's only a few (10 rows).\n",
    "\n",
    "\n",
    "3. Text\n",
    "- Only one real news has empty text, which is seemingly a graphic contents only.\n",
    "- **3.3% of fake news don't have texts**, and they were mostly **video**.\n",
    "\n",
    "\n",
    "4. Other\n",
    "- Unfortunately, this dataset doesn't contain the name of **author**. Empty or fake author name could be a strong feature which yield high precition.\n",
    "- Interesting observation about **names** is that in the real news, they are either \"title+full name\" or \"last name\", whereas in fake news, they are sometimes **just full names without title**. It's not trivial to extract names and position, so let's keep it as a note for now.\n",
    "- From selective examples, fake news show more **website addresses** or **social media account** as their **source**. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop rows with unreliable data\n",
    "df1.drop(df1[df1.title.str.split().str.len()<3].index.tolist(), axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate cleaning and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# check usage of digital source\n",
    "print(df0[df0.text.str.contains('@.', regex= True, na=False)].text.count())\n",
    "print(df1[df1.text.str.contains('@.', regex= True, na=False)].text.count())\n",
    "print(df0[df0.text.str.contains('https?://.', regex= True, na=False)].text.count())\n",
    "print(df1[df1.text.str.contains('https?://.', regex= True, na=False)].text.count())\n",
    "\n",
    "# exclude articles about Twitter\n",
    "print(df0[df0.text.str.contains('@.', regex= True, na=False) & (~df0.text.str.contains('Twitter', regex= False, na=False))].text.count())\n",
    "print(df1[df1.text.str.contains('@.', regex= True, na=False) & (~df1.text.str.contains('Twitter', regex= False, na=False))].text.count())\n",
    "\n",
    "#print(df0.iloc[31][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation about digital source\n",
    "\n",
    "**Fake news** use **social media account (@..)** and **website (http(s)://...)** as source much **more often**.\n",
    "\n",
    "1. @\n",
    "- 282 real news and 6312 fake news contains social media account.\n",
    "- **Real news** use social media account as source, mostly when the news **topic is about Twitter post**.\n",
    "- Including \"Twitter\" in the row selection, only 31 of real news contains @, whereas 4038 of fake news have it.\n",
    "\n",
    "2. http(s)://\n",
    "- 3290 of fake news have website address in the article.\n",
    "- Non of real news have website address in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace digital sources\n",
    "df0.replace(to_replace='@.', value='social_media_account', regex=True, inplace=True)\n",
    "df1.replace(to_replace='@.', value='social_media_account', regex=True, inplace=True)\n",
    "df0.replace(to_replace='https?://.', value='website_address', regex=True, inplace=True)\n",
    "df1.replace(to_replace='https?://.', value='website_address', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title and text sizes - before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "print(\"Average title length in number of words\")\n",
    "print(\"Real:\", df0.title.str.split().str.len().mean(),\"+-\",df0.title.str.split().str.len().std())\n",
    "print(\"Fake:\", df1.title.str.split().str.len().mean(),\"+-\",df1.title.str.split().str.len().std(),\"\\n\")\n",
    "\n",
    "print(\"Average text length in number of words\")\n",
    "print(\"Real:\", df0.text.str.split().str.len().mean(),\"+-\",df0.text.str.split().str.len().std())\n",
    "print(\"Fake:\", df1.text.str.split().str.len().mean(),\"+-\",df1.text.str.split().str.len().std(),\"\\n\")\n",
    "\n",
    "plt.hist(df0.title.str.split().str.len(), alpha=0.5, range=(0,50), bins=50)\n",
    "plt.hist(df1.title.str.split().str.len(), alpha=0.5, range=(0,50), bins=50)\n",
    "plt.title(\"Title word count\")\n",
    "\n",
    "plt.legend(['Real','Fake']) \n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.log10(df0.text.str.split().str.len()+1), alpha=0.5, range=(0,4), bins=50)\n",
    "plt.hist(np.log10(df1.text.str.split().str.len()+1), alpha=0.5, range=(0,4), bins=50)\n",
    "plt.title(\"Log10(Text word count)\")\n",
    "\n",
    "plt.legend(['Real','Fake'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation of raw text and title sizes\n",
    "- I expected that fake news has much fewer text. However, in most of cases, both real and fake news has **similar amount of raw text**, although whether they are informative or not is different story.\n",
    "- As we saw from selected samples in the above, real news has shorter title length with smaller standard deviation. That's seemingly because **briefness** is necessary for news title. **Long title length** can be a **useful feature** of fake news.\n",
    "- Real news has **bimodal shape for text word count**. Probably they have two types of news length and authors follow standard word count strictly for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent words - before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Word count without text processing, in case frequently used keywords are not an alphabet, etc\n",
    "title0 = df0.sample(1000).title.tolist()\n",
    "title1 = df1.sample(1000).title.tolist()\n",
    "text0 = df0.sample(1000).text.tolist()\n",
    "text1 = df1.sample(1000).text.tolist()\n",
    "\n",
    "def text2words(text):\n",
    "    \n",
    "    words_list = []\n",
    "    \n",
    "    for sentence in text:\n",
    "    \n",
    "        words = word_tokenize(sentence)\n",
    "    \n",
    "        for word in words:\n",
    "            words_list.append(word)\n",
    "\n",
    "    return words_list\n",
    "\n",
    "title0 = text2words(title0)\n",
    "title1 = text2words(title1)\n",
    "text0 = text2words(text0)\n",
    "text1 = text2words(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "word_counter = Counter(title0)\n",
    "print('\\n===== Title, Real =====\\n')\n",
    "print(word_counter.most_common(100))\n",
    "print('\\n===== Title, Fake =====\\n')\n",
    "word_counter = Counter(title1)\n",
    "print(word_counter.most_common(100))\n",
    "print('\\n===== Text, Real =====\\n')\n",
    "word_counter = Counter(text0)\n",
    "print(word_counter.most_common(100))\n",
    "print('\\n===== Text, Fake =====\\n')\n",
    "word_counter = Counter(text1)\n",
    "print(word_counter.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words,len(stop_words))\n",
    "\n",
    "#stopword.add('can')\n",
    "#stopword.add('could')\n",
    "#stopword.add('will')\n",
    "#stopword.add('would')\n",
    "#stopword.add('must')\n",
    "#stopword.add('might')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos(pos_only):\n",
    "    \n",
    "    #word_pos = pos_tag([word])\n",
    "\n",
    "    tag = ''\n",
    "    try:\n",
    "        tag = pos_only[:2]\n",
    "    except:\n",
    "        tag = 'n'\n",
    "    \n",
    "    if tag == 'JJ':\n",
    "        tag = 'a'\n",
    "    elif tag == 'NN':\n",
    "        tag = 'n'\n",
    "    elif tag == 'RB':\n",
    "        tag = 'r'\n",
    "    elif tag == 'VB':\n",
    "        tag = 'v'\n",
    "    else:\n",
    "        tag = 'n'\n",
    "        \n",
    "    return tag\n",
    "\n",
    "\n",
    "def preprocess_text(corpus):\n",
    "    \n",
    "    word_list = []\n",
    "    pos_list = []\n",
    "    keypos_list = []\n",
    "    test_list =[]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    \n",
    "    # tokenize sentence\n",
    "    sentences = sent_tokenize(corpus)\n",
    " \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # cleaning characters\n",
    "        #sentence = re.sub('\\.\\W',' ',sentence)\n",
    "        sentence = re.sub('\\.','',sentence) # keep abbreviation words\n",
    "        #sentence = re.sub('[U]\\.?[S]\\.?[A]?\\.?','usa',sentence)        \n",
    "        sentence = re.sub('US','USA',sentence)        \n",
    "        sentence = re.sub('\\w+\\*+\\w+','swear_tagged',sentence) # e.g. f*ck\n",
    "        sentence = re.sub('\\W+',' ',sentence)\n",
    "\n",
    "        # lower\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('reuters',' ',sentence)\n",
    "        sentence = re.sub('\\w*video\\w*','video',sentence)\n",
    "        \n",
    "        # cleaning short words, rarely have meaning\n",
    "        #shortword.sub('', sentence)\n",
    "        \n",
    "        #sentence = sentence[:-1]\n",
    "    \n",
    "        # word tokenize \n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            pos = pos_tag([word])\n",
    "            test_list.append(pos)\n",
    "            \n",
    "            pos_only = pos[0][1]#[:2]\n",
    "            pos_list.append(pos_only)\n",
    "            \n",
    "            \n",
    "            if word not in stop_words:\n",
    "                word_list.append(lemmatizer.lemmatize(word, pos=convert_pos(pos_only)))\n",
    "                keypos_list.append(pos_only)\n",
    "\n",
    "    return ' '.join(word_list)+' ', ' '.join(pos_list)+' ', ' '.join(keypos_list)+' '#, test_list, corpus\n",
    "\n",
    "#print(preprocess_text(df1.iloc[12][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_sample0 = 10*df1.subject.nunique()\n",
    "#n_sample1 = 10*df0.subject.nunique()\n",
    "#print(n_sample0,n_sample1)\n",
    "#df0 = pd.concat([df0[df0.subject==x].sample(n=n_sample0) for x in df0.subject.unique().tolist()])\n",
    "#df1 = pd.concat([df1[df1.subject==x].sample(n=n_sample1) for x in df1.subject.unique().tolist()])\n",
    "\n",
    "df0 = df0.sample(n=20000) #20,000\n",
    "df1 = df1.sample(n=20000)  #20,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 51.010501861572266\n",
      "time : 79.71051287651062\n",
      "time : 1342.7784399986267\n",
      "time : 1497.703408241272\n"
     ]
    }
   ],
   "source": [
    "# Takes time \n",
    "# 150s per n=1000, expect 1h for n=20k\n",
    "\n",
    "start = time.time()\n",
    "df0[['title_lm','title_pos','title_lmpos']] = df0.apply(lambda x: preprocess_text(x.title), axis=1, result_type='expand')\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "df1[['title_lm','title_pos','title_lmpos']] = df1.apply(lambda x: preprocess_text(x.title), axis=1, result_type='expand')\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "df0[['text_lm','text_pos','text_lmpos']] = df0.apply(lambda x: preprocess_text(x.text), axis=1, result_type='expand')\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "df1[['text_lm','text_pos','text_lmpos']] = df1.apply(lambda x: preprocess_text(x.text), axis=1, result_type='expand')\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "lst = []\n",
    "\n",
    "print('=======Real=========')\n",
    "for x in df0.subject.unique().tolist():\n",
    "    print('\\n',x,'\\n')\n",
    "    print(df0[df0.subject==x].head(10).title)\n",
    "    print(df0[df0.subject==x].head(10).title_lm)\n",
    "    \n",
    "    word_counter = Counter(df0[df0.subject==x].title_lm.sum().split())\n",
    "    lst.append(['Real',x,word_counter])\n",
    "    \n",
    "print('=======Fake=========')    \n",
    "for x in df1.subject.unique().tolist():\n",
    "    print('\\n',x,'\\n')\n",
    "    print(df1[df1.subject==x].head(10).title)\n",
    "    print(df1[df1.subject==x].head(10).title_lm)\n",
    "    \n",
    "    word_counter = Counter(df1[df1.subject==x].title_lm.sum().split())\n",
    "    lst.append(['Fake',x,word_counter])\n",
    "    \n",
    "for i in range(len(lst)):\n",
    "    \n",
    "    pline(lst[i][0])\n",
    "    pline(lst[i][1])\n",
    "    print(lst[i][2].most_common(20))\n",
    "\n",
    "lst = []\n",
    "\n",
    "\n",
    "word_counter = Counter(df0.title_lm.sum().split())\n",
    "lst.append(['Real','title',word_counter.most_common(30)])\n",
    "word_counter = Counter(df1.title_lm.sum().split())\n",
    "lst.append(['Fake','title',word_counter.most_common(30)])\n",
    "word_counter = Counter(df0.text_lm.sum().split())\n",
    "lst.append(['Real','text',word_counter.most_common(30)])\n",
    "word_counter = Counter(df1.text_lm.sum().split())\n",
    "lst.append(['Fake','text',word_counter.most_common(30)])\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    pline(lst[i][0]+', '+lst[i][1])\n",
    "    print(lst[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings of fake news characteristics from processed text\n",
    "\n",
    "- **\"Video\", \"Watch\", or \"Image\"** on **title**, probably in order to make readers to click.\n",
    "- **first names** are shown in the title, and use first names often in the text.\n",
    "- Some words closely relate to **discrimination based on demography** are shown often, however, I'll take a closer look later.\n",
    "- **\"Boiler, room ep, sunday, episode\"** in several irrelavent subject category. I don't know if I can trust subject labels here. They seemed to be envolved in **TV shows**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df0.to_csv('df0.csv',index=False)\n",
    "df1.to_csv('df1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
