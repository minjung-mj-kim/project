{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04be33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ecafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freq_utils import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb39b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('data/TrueOrganized.csv')\n",
    "df1 = pd.read_csv('data/FakeOrganized.csv')\n",
    "df0['label'] = 0\n",
    "df1['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69af2369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>pos</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>cleaned_pos</th>\n",
       "      <th>minimal_words</th>\n",
       "      <th>org_title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14553</th>\n",
       "      <td>14553</td>\n",
       "      <td>Putin, in decree, says Russia's armed forces are 1.9 million-strong</td>\n",
       "      <td>[('Putin', 'NNP'), (',', ','), ('in', 'IN'), ('decree', 'NN'), (',', ','), ('says', 'VBZ'), ('Russia', 'NNP'), (\"'s\", 'POS'), ('armed', 'JJ'), ('forces', 'NNS'), ('are', 'VBP'), ('1.9', 'CD'), ('m...</td>\n",
       "      <td>putin in decree says russia s armed forces are 1 . 9 million strong</td>\n",
       "      <td>NNP IN NN VBZ NNP POS JJ NNS VBP CD JJ</td>\n",
       "      <td>putin decree say russia armed force million strong</td>\n",
       "      <td>Putin, in decree, says Russia's armed forces are 1.9 million-strong</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  \\\n",
       "14553       14553   \n",
       "\n",
       "                                                                     title  \\\n",
       "14553  Putin, in decree, says Russia's armed forces are 1.9 million-strong   \n",
       "\n",
       "                                                                                                                                                                                                           pos  \\\n",
       "14553  [('Putin', 'NNP'), (',', ','), ('in', 'IN'), ('decree', 'NN'), (',', ','), ('says', 'VBZ'), ('Russia', 'NNP'), (\"'s\", 'POS'), ('armed', 'JJ'), ('forces', 'NNS'), ('are', 'VBP'), ('1.9', 'CD'), ('m...   \n",
       "\n",
       "                                                             cleaned_words  \\\n",
       "14553  putin in decree says russia s armed forces are 1 . 9 million strong   \n",
       "\n",
       "                                  cleaned_pos  \\\n",
       "14553  NNP IN NN VBZ NNP POS JJ NNS VBP CD JJ   \n",
       "\n",
       "                                            minimal_words  \\\n",
       "14553  putin decree say russia armed force million strong   \n",
       "\n",
       "                                                                 org_title  \\\n",
       "14553  Putin, in decree, says Russia's armed forces are 1.9 million-strong   \n",
       "\n",
       "      label  \n",
       "14553  True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df0.sample(1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da35a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = train_dev_test_split([df0, df1], m=1000, class_column='label', \n",
    "                                    class_balance=True, r_dev=0.2, r_test=0.2, rand_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c076cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_NB(train,dev,test,Xname='title',Yname='label'):\n",
    "    \n",
    "    train = pd.concat([train,dev])\n",
    "    \n",
    "    X_train = train[Xname].tolist()\n",
    "    Y_train = train[Yname].tolist()\n",
    "\n",
    "    X_test = test[Xname].tolist()\n",
    "    Y_test = test[Yname].tolist()\n",
    "    \n",
    "    counter = CountVectorizer()\n",
    "\n",
    "    counter.fit(X_train+X_test)\n",
    "\n",
    "    train_counts = counter.transform(X_train)\n",
    "    test_counts = counter.transform(X_test)\n",
    "\n",
    "    #print(counter.vocabulary_)\n",
    "\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(train_counts,Y_train)\n",
    "    \n",
    "    predict = classifier.predict(test_counts)\n",
    "    \n",
    "    proba = classifier.predict_proba(test_counts)\n",
    "    \n",
    "    return predict, Y_test, proba, classifier, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11e68b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_FNN(input_shape, word_to_vec_map, word_to_index, n_class=2, trainable=False):\n",
    "    '''\n",
    "    input_shape: (max_len,)\n",
    "    word_to_vec_map: word to embedding vector dictionary\n",
    "    word_to_index: word to index dictionary\n",
    "\n",
    "    return model\n",
    "\n",
    "    then\n",
    "    X: Indices of a sentence (m, max_len)\n",
    "    Y: Class probability, one hot vector (m, # classes)\n",
    "    '''\n",
    "\n",
    "    # Input layer\n",
    "    X_indices = tfl.Input(shape=input_shape, dtype='int32')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, trainable=trainable)\n",
    "    X = embedding_layer(X_indices)   \n",
    "\n",
    "    # Take average\n",
    "    # Get embedding vector dimension\n",
    "    emb_dim = X.shape[2]\n",
    "    # Make a list from slice\n",
    "    X_avg = [ X[:,:,i] for i in range(emb_dim) ]\n",
    "    # Take average of embedding vector\n",
    "    X = tf.keras.layers.Average()(X_avg)\n",
    "\n",
    "    # Masking layer\n",
    "    # skip zero vector words\n",
    "    X = tfl.Masking(mask_value=0.)(X)\n",
    "\n",
    "    # Linear+ReLu layer\n",
    "    X = tfl.Dense(units = 128, activation='relu')(X)\n",
    "    X = tfl.Dropout(rate = 0.4)(X)  \n",
    "\n",
    "    # Linear+ReLu layer\n",
    "    X = tfl.Dense(units = 64, activation='relu')(X)\n",
    "    X = tfl.Dropout(rate = 0.4)(X) \n",
    "\n",
    "    # Linear+ReLu layer\n",
    "    X = tfl.Dense(units = 32, activation='relu')(X)\n",
    "    X = tfl.Dropout(rate = 0.2)(X) \n",
    "\n",
    "    # Linear+Softmax layer\n",
    "    # Output: y (m, # classes), probability of each class\n",
    "    X = tfl.Dense(units = n_class, activation='softmax')(X)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.models.Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "edce9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(input_shape, word_to_vec_map, word_to_index, n_class=2, trainable=False):\n",
    "    '''\n",
    "    input_shape: (max_len,)\n",
    "    word_to_vec_map: word to embedding vector dictionary\n",
    "    word_to_index: word to index dictionary\n",
    "\n",
    "    return model\n",
    "\n",
    "    then\n",
    "    X: Indices of a sentence (m, max_len)\n",
    "    Y: Class probability, one hot vector (m, # classes)\n",
    "    '''\n",
    "\n",
    "    # Input layer\n",
    "    X_indices = tfl.Input(shape=input_shape, dtype='int32')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, trainable=trainable)\n",
    "    X = embedding_layer(X_indices)   \n",
    "\n",
    "    # Masking layer\n",
    "    # skip zero vector words\n",
    "    X = tfl.Masking(mask_value=0.)(X)\n",
    "\n",
    "    # LSTM layer\n",
    "    # Output: a[1] (m, max_len, 128 hidden unit), batch of sequences\n",
    "    X = tfl.LSTM(units = 128, return_sequences= True)(X)\n",
    "    X = tfl.Dropout(rate = 0.5 )(X) \n",
    "\n",
    "    # LSTM layer\n",
    "    # Output: a[2]<max_len> (m, 128 hidden unit)\n",
    "    X = tfl.LSTM(units = 128)(X)\n",
    "    X = tfl.Dropout(rate = 0.5)(X)  \n",
    "\n",
    "    # Linear layer\n",
    "    # Output: a[3] (m, # classes)\n",
    "    X = tfl.Dense(units = n_class)(X)\n",
    "\n",
    "    # Softmax layer\n",
    "    # Output: y (m, # classes), probability of each class\n",
    "    X = tfl.Activation('softmax')(X)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.models.Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b812e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_language_model(model, train, dev, test, Xname='org_title',Yname='label', max_len=20, n_class=2, \\\n",
    "                                                    epochs = 20, batch_size = 32, \\\n",
    "                                                    patience=2, trainable=False):\n",
    "      \n",
    "    model = model((max_len,), word_to_vec_map, word_to_index, n_class, trainable)\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "    \n",
    "    history = False\n",
    "    \n",
    "    if patience :\n",
    "        history = model.fit(X_train_indices, Y_train_oh, \n",
    "                            epochs = epochs, batch_size = batch_size, shuffle=True, \n",
    "                            validation_data=(X_dev_indices, Y_dev_oh),\n",
    "                            callbacks=[es])\n",
    "    else:\n",
    "        history = model.fit(X_train_indices, Y_train_oh, \n",
    "                            epochs = epochs, batch_size = batch_size, shuffle=True, \n",
    "                            validation_data=(X_dev_indices, Y_dev_oh))\n",
    "        \n",
    "        \n",
    "    proba = model.predict(X_test_indices)\n",
    "    predict = [np.argmax(proba[i]) for i in range(len(proba))]\n",
    "    \n",
    "    return predict, Y_test, proba, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74ec3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = get_pretrained_embedding()\n",
    "\n",
    "_, X_train, X_train_indices, Y_train, Y_train_oh = dataframe_to_arrays(train, word_to_index, max_len)\n",
    "_, X_dev,   X_dev_indices,   Y_dev,   Y_dev_oh   = dataframe_to_arrays(dev, word_to_index, max_len)\n",
    "indx_test, X_test,  X_test_indices,  Y_test,  Y_test_oh  = dataframe_to_arrays(test, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d50de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = simple_NB(train,dev,test,Xname='title',Yname='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93dee77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.9381443298969072\n",
      "0.900990099009901\n",
      "0.9191919191919191\n"
     ]
    }
   ],
   "source": [
    "test_labels = res[1]\n",
    "predict = res[0]\n",
    "\n",
    "#print(res)\n",
    "\n",
    "print(accuracy_score(test_labels, predict))\n",
    "print(precision_score(test_labels, predict))\n",
    "print(recall_score(test_labels, predict))\n",
    "print(f1_score(test_labels, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 20, 50)            20000050  \n",
      "                                                                 \n",
      " masking_3 (Masking)         (None, 20, 50)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 20, 128)           91648     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 20, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,223,540\n",
      "Trainable params: 223,490\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 11s 207ms/step - loss: 0.5186 - accuracy: 0.7183 - val_loss: 0.3856 - val_accuracy: 0.8350\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.3548 - accuracy: 0.8417 - val_loss: 0.3432 - val_accuracy: 0.8650\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.3003 - accuracy: 0.8767 - val_loss: 0.3181 - val_accuracy: 0.8650\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.2608 - accuracy: 0.8900 - val_loss: 0.4318 - val_accuracy: 0.8100\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2789 - accuracy: 0.8783 - val_loss: 0.2884 - val_accuracy: 0.8700\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.2255 - accuracy: 0.9033 - val_loss: 0.2875 - val_accuracy: 0.8650\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.2053 - accuracy: 0.9067 - val_loss: 0.3189 - val_accuracy: 0.8800\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.2045 - accuracy: 0.9033 - val_loss: 0.2753 - val_accuracy: 0.8800\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1454 - accuracy: 0.9433 - val_loss: 0.3323 - val_accuracy: 0.8900\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1407 - accuracy: 0.9433 - val_loss: 0.3482 - val_accuracy: 0.8750\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "res = run_language_model(model_LSTM, train, dev, test, Xname='cleaned_words',Yname='label', \\\n",
    "                         max_len=max_len, n_class=2, \\\n",
    "                         epochs = 20, batch_size = 32, \\\n",
    "                         patience=2, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30e67671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595\n",
      "0.6612903225806451\n",
      "0.40594059405940597\n",
      "0.5030674846625767\n"
     ]
    }
   ],
   "source": [
    "test_labels = res[1]\n",
    "predict = res[0]\n",
    "\n",
    "print(accuracy_score(test_labels, predict))\n",
    "print(precision_score(test_labels, predict))\n",
    "print(recall_score(test_labels, predict))\n",
    "print(f1_score(test_labels, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dac76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
